{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "zmkbv5gak4pfl64gcuvv",
   "authorId": "507732859711",
   "authorName": "ADMIN",
   "authorEmail": "adam.neel@snowflake.com",
   "sessionId": "ec4c215e-a892-4f6a-99de-8005cff62a04",
   "lastEditTime": 1754663323362
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9923d8-945b-4b4b-8d37-3a48807cafa3",
   "metadata": {
    "name": "MD_intro",
    "collapsed": false
   },
   "source": "# üß†  Document Q&A ‚Äì Multimodal Document Pipeline (Streamlit UI)\n\nThis script powers an interactive Streamlit app that answers technical questions about Seclock‚Äôs door hardware documents using **multimodal AI**‚Äîcombining vector search, LLM reasoning, and image validation via Snowflake Cortex.\n\n## üîß High-Level Features\n\n- üì• **Document Ingestion & Processing**\n  - Splits PDFs into per-page images and text using `parse_document()`.\n  - Stores images in a Snowflake stage and text/metadata in structured tables.\n  - Generates image embeddings using `embed_image_1024`.\n---\n- üîç **Semantic Search & Retrieval**\n  - Converts user questions into temporary query images to leverage image embeddings.\n  - Embeds the image and performs vector search via Cortex Search Service.\n  - Retrieves top matching enriched chunks (text + metadata + image reference).\n---\n- üß† **LLM-Based Text Answering**\n  - Feeds retrieved context into Claude (`claude-3-7-sonnet`) to generate:\n    - Direct answers\n    - Confidence scores\n    - Justifications\n    - Markdown-linked citations (`[Document - page X](presigned_url)`)\n---\n- üñºÔ∏è **Image-Based Reasoning**\n  - Identifies cited pages from the text answer and filters matching images.\n  - Submits images asynchronously to Cortex for answer validation.\n  - Uses image metadata and visual context to extract direct answers or critiques.\n---\n- üß™ **Answer Synthesis**\n  - Combines text and image-based answers into a final, human-readable response.\n  - Rewrites for clarity, accuracy, and directness.\n  - Prioritizes newer or more reliable sources and flags conflicting evidence.\n  - Provides full citation trail with links to specific document pages.\n---\n- üîó **Embedding & Retrieval Enrichment**\n  - Merges document metadata, summaries, page-level chunks, and vectors into a retrieval-optimized format.\n  - Allows precise filtering and future expansion (e.g., by product line or brand).\n---\n- üí¨ **Streamlit Chat UI**\n  - Provides an interactive chatbot interface with step-by-step progress feedback.\n  - Includes expandable debug sections for:\n    - Source documents used\n    - Text answer reasoning\n    - Image-based critiques\n    - LLM prompt previews\n"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "PY_imports"
   },
   "source": "# Import python packages\nimport os\nimport sys\nimport json\nimport shutil\nimport datetime\nimport re\nimport time\nimport hashlib\nfrom difflib import SequenceMatcher\nimport tempfile\nfrom textwrap import dedent\nimport streamlit as st\nfrom PIL import Image, ImageDraw, ImageFont\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import List\nfrom typing import Tuple\nimport snowflake.snowpark.session as session\nimport pdfplumber\nimport PyPDF2\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.core import Root\nfrom snowflake.cortex import complete, CompleteOptions\nsp_session = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1cc14c85-301e-4f5a-b7f7-059355aa2230",
   "metadata": {
    "name": "MD_pdf_to_image",
    "collapsed": false
   },
   "source": "## üìÑ PDF Preprocessing Pipeline for Document Analysis\n\nPreprocesses PDFs stored in a Snowflake stage, preparing them for downstream AI document analysis. It lists all PDF files in a specified input stage, downloads each file temporarily, and performs two key operations: \n\n1. Splitting the PDF into individual pages and uploading each as a separate PDF file\n2. Converting each page into a high-resolution image, optionally scaled to a maximum dimension, and uploading the images back to a specified output stage. "
  },
  {
   "cell_type": "code",
   "id": "0ad96295-d4e4-43d7-8ab5-62f685fef26f",
   "metadata": {
    "language": "python",
    "name": "PY_pdf_to_image"
   },
   "outputs": [],
   "source": "def print_info(msg: str) -> None:\n    \"\"\"Print info message\"\"\"\n    print(f\"INFO: {msg}\", file=sys.stderr)\n\n\ndef print_error(msg: str) -> None:\n    \"\"\"Print error message\"\"\"\n    print(f\"ERROR: {msg}\", file=sys.stderr)\n    if hasattr(st, \"error\"):\n        st.error(msg)\n\n\ndef print_warning(msg: str) -> None:\n    \"\"\"Print warning message\"\"\"\n    print(f\"WARNING: {msg}\", file=sys.stderr)\n\n\n@dataclass\nclass Config:\n    input_stage: str = \"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/\"\n    output_stage: str = (\n        \"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/PARSED/\"  # Base output stage without subdirectories\n    )\n    input_path: str = \"pre_processed\"\n    output_pdf_path: str = \"paged_pdf\"\n    output_image_path: str = \"paged_image\"\n    allowed_extensions: List[str] = None\n    max_dimension: int = 1500  # Maximum dimension in pixels before scaling\n    dpi: int = 300  # Default DPI for image conversion\n\n    def __post_init__(self):\n        if self.allowed_extensions is None:\n            self.allowed_extensions = [\".pdf\"]\n\n\nclass PDFProcessingError(Exception):\n    \"\"\"Base exception for PDF processing errors\"\"\"\n\n\nclass FileDownloadError(PDFProcessingError):\n    \"\"\"Raised when file download fails\"\"\"\n\n\nclass PDFConversionError(PDFProcessingError):\n    \"\"\"Raised when PDF conversion fails\"\"\"\n\n\n@contextmanager\ndef managed_temp_file(suffix: str = None) -> str:\n    \"\"\"Context manager for temporary file handling\"\"\"\n    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n    try:\n        yield temp_file.name\n    finally:\n        # Don't delete the file immediately, let the caller handle cleanup\n        pass\n\n\ndef cleanup_temp_file(file_path: str) -> None:\n    \"\"\"Clean up a temporary file\"\"\"\n    try:\n        if os.path.exists(file_path):\n            os.unlink(file_path)\n    except OSError as e:\n        print_warning(f\"Failed to delete temporary file {file_path}: {e}\")\n\n\ndef list_pdf_files(session: session.Session, config: Config) -> List[dict]:\n    \"\"\"List all PDF files in the source stage\"\"\"\n    try:\n        # Use LIST command instead of DIRECTORY function\n        query = f\"\"\"\n        LIST {config.input_stage}\n        \"\"\"\n\n        file_list = session.sql(query).collect()\n\n        # Filter for PDF files\n        pdf_files = []\n        for file_info in file_list:\n            full_path = file_info[\"name\"]\n            # Extract just the filename from the full path\n            file_name = os.path.basename(full_path)\n\n            if any(\n                file_name.lower().endswith(ext) for ext in config.allowed_extensions\n            ):\n                pdf_files.append(\n                    {\n                        \"RELATIVE_PATH\": file_name,  # Use just the filename\n                        \"FULL_STAGE_PATH\": full_path,  # Use full path for download\n                        \"SIZE\": file_info[\"size\"] if \"size\" in file_info else 0,\n                    }\n                )\n\n        print_info(f\"Found {len(pdf_files)} PDF files in the stage\")\n        return pdf_files\n    except Exception as e:\n        print_error(f\"Failed to list files: {e}\")\n        raise\n\n\ndef download_file_from_stage(\n    session: session.Session, file_path: str, config: Config\n) -> str:\n    \"\"\"Download a file from stage using session.file.get\"\"\"\n    # Create a temporary directory\n    temp_dir = tempfile.mkdtemp()\n    try:\n        # Ensure there are no double slashes in the path\n        stage_path = f\"{config.input_stage.rstrip('/')}/{file_path.lstrip('/')}\"\n\n        # Get the file from stage\n        get_result = session.file.get(stage_path, temp_dir)\n        if not get_result or get_result[0].status != \"DOWNLOADED\":\n            raise FileDownloadError(f\"Failed to download file: {file_path}\")\n\n        # Construct the local path where the file was downloaded\n        local_path = os.path.join(temp_dir, os.path.basename(file_path))\n        if not os.path.exists(local_path):\n            raise FileDownloadError(f\"Downloaded file not found at: {local_path}\")\n\n        return local_path\n    except Exception as e:\n        print_error(f\"Error downloading {file_path}: {e}\")\n        # Clean up the temporary directory\n        try:\n            import shutil\n\n            shutil.rmtree(temp_dir)\n        except Exception as cleanup_error:\n            print_warning(f\"Failed to clean up temporary directory: {cleanup_error}\")\n        raise FileDownloadError(f\"Failed to download file: {e}\")\n\n\ndef upload_file_to_stage(\n    session: session.Session, file_path: str, output_path: str, config: Config\n) -> str:\n    \"\"\"Upload file to the output stage\"\"\"\n    try:\n        # Get the directory and filename from the output path\n        output_dir = os.path.dirname(output_path)\n        base_name = os.path.basename(output_path)\n\n        # Create the full stage path with subdirectory\n        stage_path = f\"{config.output_stage.rstrip('/')}/{output_dir.lstrip('/')}\"\n\n        # Read the content of the original file\n        with open(file_path, \"rb\") as f:\n            file_content = f.read()\n\n        # Create a new file with the correct name\n        temp_dir = tempfile.gettempdir()\n        temp_file_path = os.path.join(temp_dir, base_name)\n\n        # Write the content to the new file\n        with open(temp_file_path, \"wb\") as f:\n            f.write(file_content)\n\n        # Upload the file using session.file.put with compression disabled\n        put_result = session.file.put(\n            temp_file_path, stage_path, auto_compress=False, overwrite=True\n        )\n\n        # Check upload status\n        if not put_result or len(put_result) == 0:\n            raise Exception(f\"Failed to upload file: {base_name}\")\n\n        if put_result[0].status not in [\"UPLOADED\", \"SKIPPED\"]:\n            raise Exception(f\"Upload failed with status: {put_result[0].status}\")\n\n        # Clean up the temporary file\n        if os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n\n        return f\"Successfully uploaded {base_name} to {stage_path}\"\n    except Exception as e:\n        print_error(f\"Error uploading file: {e}\")\n        raise\n\n\ndef process_pdf_files(config: Config) -> None:\n    \"\"\"Main process to orchestrate the PDF splitting\"\"\"\n    try:\n        session = get_active_session()\n        pdf_files = list_pdf_files(session, config)\n\n        for file_info in pdf_files:\n            file_path = file_info[\"RELATIVE_PATH\"]\n            print_info(f\"Processing: {file_path}\")\n\n            try:\n                # Download the PDF file\n                local_pdf_path = download_file_from_stage(session, file_path, config)\n\n                # Get base filename without extension\n                base_name = os.path.splitext(os.path.basename(file_path))[0]\n\n                # Extract individual PDF pages\n                with open(local_pdf_path, \"rb\") as file:\n                    pdf_reader = PyPDF2.PdfReader(file)\n                    num_pages = len(pdf_reader.pages)\n                    print_info(f\"Converting PDF to {num_pages} pages of PDFs\")\n\n                    for i in range(num_pages):\n                        page_num = i + 1\n                        s3_pdf_output_path = (\n                            f\"{config.output_pdf_path}/{base_name}_page_{page_num}.pdf\"\n                        )\n                        pdf_writer = PyPDF2.PdfWriter()\n                        pdf_writer.add_page(pdf_reader.pages[i])\n                        temp_file = tempfile.NamedTemporaryFile(\n                            delete=False, suffix=\".pdf\"\n                        )\n                        local_pdf_tmp_file_name = temp_file.name\n                        with open(local_pdf_tmp_file_name, \"wb\") as output_file:\n                            pdf_writer.write(output_file)\n                        \n                        upload_file_to_stage(\n                            session, local_pdf_tmp_file_name, s3_pdf_output_path, config\n                        )\n                        cleanup_temp_file(local_pdf_tmp_file_name)\n                            \n                # Convert PDF to images                \n                with pdfplumber.open(local_pdf_path) as pdf:\n                    print_info(f\"Converting PDF to {len(pdf.pages)} images\")\n                    for i, page in enumerate(pdf.pages):\n                        page_num = i + 1\n                        # Get page dimensions\n                        width = page.width\n                        height = page.height\n\n                        # Determine if scaling is needed\n                        max_dim = max(width, height)\n                        if max_dim > config.max_dimension:\n                            # Calculate scale factor to fit within max_dimension\n                            scale_factor = config.max_dimension / max_dim\n                            width = int(width * scale_factor)\n                            height = int(height * scale_factor)\n\n                        img = page.to_image(resolution=config.dpi)\n                        temp_file = tempfile.NamedTemporaryFile(\n                            delete=False, suffix=\".png\"\n                        )\n                        local_image_tmp_file_name = temp_file.name\n                        img.save(local_image_tmp_file_name)\n\n                        s3_image_output_path = (\n                            f\"{config.output_image_path}/{base_name}_page_{page_num}.png\"\n                        )\n                        \n                        upload_file_to_stage(\n                            session, local_image_tmp_file_name, s3_image_output_path, config\n                        )\n                        cleanup_temp_file(local_image_tmp_file_name)\n                        \n                # Clean up the original downloaded file\n                cleanup_temp_file(local_pdf_path)\n\n            except Exception as e:\n                print_error(f\"Error processing {file_path}: {e}\")\n                continue\n\n    except Exception as e:\n        print_error(f\"Fatal error in process_pdf_files: {e}\")\n        raise",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7710c338-9693-4d20-8459-3eb567595a0a",
   "metadata": {
    "language": "python",
    "name": "PY_run_pdf_to_image"
   },
   "outputs": [],
   "source": "config = Config(dpi=300)\nprocess_pdf_files(config)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96ff282e-8bbc-4910-be87-f74345c98a7e",
   "metadata": {
    "name": "MD_image_preview",
    "collapsed": false
   },
   "source": "## üîç Document Image Preview\n\nTo check everything has been processed as planned, we can look at an image representing a page from the PDFs."
  },
  {
   "cell_type": "code",
   "id": "4eb451f8-d784-47fe-be85-0697507bc2b0",
   "metadata": {
    "language": "python",
    "name": "PY_image_preview"
   },
   "outputs": [],
   "source": "image_path = f\"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/PARSED/paged_image/2023-factbook_page_27.png\"\ntry:\n    image_st = sp_session.file.get_stream( image_path,decompress=False)\nexcept Exception as e:\n    print(\"failed to initialize file stream:\", e)\n    \nimage = image_st.read()\nst.image(image)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d7bbcfc-b7b4-4550-b181-ab9df1e37891",
   "metadata": {
    "language": "sql",
    "name": "SQL_use_role"
   },
   "outputs": [],
   "source": "use role accountadmin",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd639952-f63d-4534-a7ca-13b5c898dbdd",
   "metadata": {
    "name": "MD_image_embedding",
    "collapsed": false
   },
   "source": "## üß† Batch Image Embedding with Cortex and Snowpark\n\nThis workflow performs batch image embedding using a Python stored procedure\n\n1. **Identify Unprocessed Images**\n   A temporary table (`limit_directory_table`) is created by listing all image files in the stage (`@utils.ai.stock_ikb_documents/paged_image/`) and filtering out those already embedded in the `output_vector_table`.\n\n2. **Assign Row Numbers for Batching**\n   Each unprocessed image file is assigned a `row_number()` so batches can be defined by row ranges (`start_rn` to `end_rn`).\n\n3. **Define Embedding Procedure**\n   A Python stored procedure `run_image_embedding_batch(start_rn, end_rn)` is created. It:\n\n   * Reads a batch of image files from the temporary table.\n   * Extracts file and metadata (e.g. file name, page number).\n   * Computes an image embedding using `snowflake.cortex.embed_image_1024` with the `voyage-multimodal-3` model.\n   * Saves the embeddings to `output_vector_table`.\n\n4. **Queue Up Batch Jobs**\n   The total number of batches is calculated, and a list of SQL `CALL` statements is built, one per batch.\n\n5. **Run Jobs Concurrently**\n   A loop manages job execution with up to 5 concurrent asynchronous jobs at a time. Each job is submitted using `.collect_nowait()` and polled until it completes.\n\n6. **Monitor and Retry**\n   Each batch is logged upon completion or failure, and the loop continues until all batches are processed.\n\nThis setup allows high-throughput embedding of images inside Snowflake, using Cortex's multimodal capabilities with minimal manual orchestration.\n"
  },
  {
   "cell_type": "code",
   "id": "9a214a51-92d6-42b5-854f-a7da48b04402",
   "metadata": {
    "language": "sql",
    "name": "SQL_image_embedding_sproc"
   },
   "outputs": [],
   "source": "create or replace procedure run_image_embedding_batch(start_rn int, end_rn int)\nreturns string\nlanguage python\nruntime_version = 3.9\npackages = ('snowflake-snowpark-python')\nhandler = 'embed_handler'\nAS\n$$\ndef embed_handler(session, start_rn, end_rn):\n    df = session.sql(f'''\n        select\n            concat('paged_image/', split_part(relative_path, '/', -1)) as file_name,\n            regexp_substr(file_name, 'paged_image/(.*)\\\\.png$', 1, 1, 'e', 1) as paged_file_name,\n            split_part(paged_file_name, '_page_', 0) as original_file_name,\n            split_part(paged_file_name, '_page_', 2)::int as page_number,\n            '@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/' as stage_prefix,\n            to_file(file_url)  as image_file,\n            AI_EMBED(\n                'voyage-multimodal-3', \n                '@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/'||concat('PARSED/paged_image/', split_part(relative_path, '/', -1))\n            ) as image_vector\n        from limit_directory_table\n        where rn between {start_rn} and {end_rn}\n    ''')\n    ## print(df.columns)\n    df.write.save_as_table(\"OUTPUT_VECTOR_TABLE\", mode=\"append\") ##change this to append\n    return f\"Embedded and saved RN {start_rn} to {end_rn}\"\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0cea428-fea0-428e-b738-a52ecbe8c43c",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "--call run_image_embedding_batch(start_rn int, end_rn int)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a1b1150-0887-4877-9a1d-5f525b54f7fc",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "select get_ddl('table','output_vector_table');\ncreate or replace TABLE OUTPUT_VECTOR_TABLE (\n\tFILE_NAME VARCHAR(16777216),\n\tPAGED_FILE_NAME VARCHAR(16777216),\n\tORIGINAL_FILE_NAME VARCHAR(16777216),\n\tPAGE_NUMBER NUMBER(38,0),\n\tSTAGE_PREFIX VARCHAR(43) NOT NULL,\n\tIMAGE_FILE FILE,\n\tIMAGE_VECTOR VECTOR(FLOAT, 1024)\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5009481b-9c5d-4ea2-81fc-2a8380b8190a",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "select * from output_vector_table",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f45bd12-76ab-4051-b716-6bb080b97ec5",
   "metadata": {
    "language": "python",
    "name": "PY_image_embedding"
   },
   "outputs": [],
   "source": "BATCH_SIZE = 10\nMAX_CONCURRENT = 5\n\n# 1. Create LIMIT_DIRECTORY_TABLE if not exists\nsp_session.sql(\"\"\"\n    create or replace temporary table limit_directory_table as\n    select\n        *,\n        row_number() over (order by relative_path) as rn\n    from\n        directory(@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO)\n    where\n        relative_path like '%paged_image/%'\n        \n\"\"\").collect()\n\n    # create or replace temporary table limit_directory_table as\n    # select\n    #     *,\n    #     row_number() over (order by relative_path) as rn\n    # from\n    #     directory(@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO)\n    # where\n    #     relative_path like '%paged_image/%'\n    #     and\n    #     relative_path not in (\n    #         select file_name from output_vector_table\n\n### The above sql command should be used if we want to only load deltas\n    #     \n\n# 2. Get total batches\nmax_rn = sp_session.sql(\"select max(rn) AS max_rn from limit_directory_table\").collect()[0][\"MAX_RN\"]\ntotal_batches = (max_rn + BATCH_SIZE - 1) // BATCH_SIZE\n\n# 3. Prepare all batch configs\nbatch_queue = []\nfor i in range(total_batches):\n    start_rn = i * BATCH_SIZE + 1\n    end_rn = min((i + 1) * BATCH_SIZE, max_rn)\n    label = f\"Batch {i+1}: RN {start_rn}-{end_rn}\"\n    sql = f\"call run_image_embedding_batch({start_rn}, {end_rn})\"\n    batch_queue.append((sql, label))\n\n# 4. Loop with max 5 concurrent jobs\nactive_jobs = []\n\nwhile batch_queue or active_jobs:\n    # Launch jobs if we have capacity\n    while batch_queue and len(active_jobs) < MAX_CONCURRENT:\n        sql, label = batch_queue.pop(0)\n        print(f\"üöÄ Submitting async job for {label}\")\n        try:\n            job = sp_session.sql(sql).collect_nowait()\n            active_jobs.append((job, label))\n        except Exception as e:\n            print(f\"‚ùå Failed to submit {label}: {e}\")\n\n    # Poll active jobs\n    for job, label in active_jobs.copy():\n        if job.is_done():\n            try:\n                result = job.result()\n                print(f\"‚úÖ {label} completed: {result}\")\n            except Exception as e:\n                print(f\"‚ùå {label} failed: {e}\")\n            active_jobs.remove((job, label))\n\n    if active_jobs:\n        time.sleep(15)\n\nprint(\"üéâ All batches processed.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1d767aa-1f24-422e-aef3-9c6ea3e13842",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "select count(*) from output_vector_table",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8dd82d8-665b-4715-8d4d-791776f593a2",
   "metadata": {
    "name": "MD_ocr",
    "collapsed": false
   },
   "source": "## üîñ Extract Text from PDF Pages\n\nThis SQL script creates a table (`pdf_pages`) that extracts and stores parsed text content from individual PDF pages:\n\n1. **Filter Input Files**\n   It queries the stage `@utils.ai.stock_ikb_documents` and filters files whose path matches the pattern `%paged_pdf/%`, meaning individual page PDFs from previously split documents.\n\n2. **Extract File Metadata**\n   For each PDF file:\n\n   * `file_name` is constructed by prefixing the relative path with `paged_pdf/`.\n   * `paged_file_name` extracts just the PDF filename using regex.\n   * `original_file_name` removes the `_page_X` suffix to get the base document name.\n   * `page_number` is parsed from the filename to track the page.\n\n3. **Generate File References**\n   The `to_file(file_url)` function creates a file object for use in Cortex functions.\n\n4. **Parse PDF Content with Cortex**\n   The `snowflake.cortex.parse_document` function is called on each page to extract its text layout. The result is cast to a string, then parsed as JSON and stored in the `pdf_text` column.\n\n5. **Output the Resulting Table**\n   The final table `pdf_pages` includes:\n\n   * File path and name metadata\n   * Page number\n   * File reference\n   * Structured PDF content parsed by Cortex\n\nThis process enables structured, searchable access to individual page-level text from large documents using Cortex's layout-aware parsing engine.\n"
  },
  {
   "cell_type": "code",
   "id": "c5cc83a2-d562-4a25-b194-05a10e451563",
   "metadata": {
    "language": "sql",
    "name": "SQL_ocr"
   },
   "outputs": [],
   "source": "create or replace table pdf_pages as\nselect\n    concat('PARSED/paged_pdf/', split_part(relative_path, '/', -1)) as file_name,\n    regexp_substr(file_name, 'PARSED/paged_pdf/(.*)\\\\.pdf$', 1, 1, 'e', 1) as paged_file_name,\n    split_part(paged_file_name, '_page_', 0) as original_file_name,\n    split_part(paged_file_name, '_page_', 2)::int as page_number,\n    '@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO' as stage_prefix,\n    to_file(file_url) as pdf_file,\n    parse_json(\n        to_varchar(\n            snowflake.cortex.parse_document(\n                '@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO',\n                file_name,\n                {'mode': 'LAYOUT'}\n            )\n        )\n    ):content as pdf_text\nfrom\n    directory(@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO)\nwhere\n    relative_path like '%paged_pdf/%'\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ee157c4-cc09-427e-b433-b74eb96de349",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "select * from pdf_pages;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db118332-e245-4ff3-b962-28c90505597f",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "select * from output_vector_table;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0ae7050-8372-48d3-a4b8-a38b06ce306a",
   "metadata": {
    "name": "MD_metadata_chunking",
    "collapsed": false
   },
   "source": "## üìñ Enriching PDF Pages with Metadata and Text Chunks for Semantic Search\n\nThis SQL pipeline creates a comprehensive table (`utils.ai.pdf_images_joined`) that combines page-level text, image embeddings, structured metadata, and semantically formatted chunks optimized for multimodal document retrieval using Snowflake Cortex.\n\n### ‚úÖ Steps:\n\n1. **üñáÔ∏è Join PDF Pages with Image Embeddings**\n\n   * Merges parsed PDF page data from `pdf_pages` with vector embeddings from `output_vector_table` via `paged_file_name`.\n---\n2. **üìÑ Select Representative Pages for Metadata**\n\n   * Uses `row_number()` to select:\n\n     * The **first 10 pages** (for coverage of typical document headers).\n     * The **last 2 pages** (often contain part indexes or summaries).\n---\n3. **üß† Generate Document-Level Metadata**\n\n   * Concatenates the selected pages‚Äô text and feeds it into `ai_complete()` (with `llama4-scout`) to extract:\n\n     * `manufacturer`\n     * `product_line`\n     * `document_type`\n     * `effective_date`, `copyright`\n     * `category`\n     * `concise_document_summary`\n---\n4. **üìù Generate Page-Level Metadata**\n\n   * Runs `ai_complete()` (with `llama4-scout`) on each page‚Äôs text to extract:\n\n     * `page_title`\n     * `concise_page_summary`\n---\n5. **üîó Join Metadata with Full Page Content**\n\n   * Combines document-level and page-level metadata with:\n\n     * Raw page text\n     * Vector embeddings\n     * File references\n---\n6. **‚úÇÔ∏è Split Pages into Chunks**\n\n   * Uses `cortex.split_text_recursive_character()` to break page text into \\~1800-character, markdown-safe blocks, ensuring semantic cohesion for chunk-level retrieval.\n---\n7. **üîç Enrich Chunks with Visual Context**\n\n   * For each chunk:\n\n     * Runs `ai_complete()` (with `llama4-maverick`) using the **full page image** and **chunk text**.\n     * Extracts structured visual context such as:\n\n       * **Page region**\n       * **Table sections or headers**\n       * **Related elements not captured in the chunk**\n     * Encourages bullet-point or key-value output grounded in visual layout.\n---\n8. **üß± Build Final Enriched Chunks**\n\n   * Combines:\n\n     * Source file\n     * Document and page metadata\n     * Chunk visual context\n     * Raw chunk text\n   * Stores final result in an `enriched_chunk` field optimized for LLM prompts and semantic indexing."
  },
  {
   "cell_type": "code",
   "id": "b905e893-f090-4b3e-aabb-6bbbc6c09b90",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": " select\n        pdfs.file_name as pdf_file_name,\n        images.file_name as image_file_name,\n        pdfs.original_file_name,\n        pdfs.page_number,\n        pdfs.pdf_file,\n        images.image_file,\n        images.image_vector,\n        pdfs.pdf_text,\n    from\n        pdf_pages as pdfs\n    join\n        output_vector_table as images\n        on\n        pdfs.paged_file_name = images.paged_file_name",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f4e2d39-332d-4abc-89f5-c132dc885868",
   "metadata": {
    "language": "sql",
    "name": "SQL_metadata_chunking"
   },
   "outputs": [],
   "source": "\ncreate or replace table pdf_images_joined as\nwith pdf_images_joined as (\n    select\n        pdfs.file_name as pdf_file_name,\n        images.file_name as image_file_name,\n        pdfs.original_file_name,\n        pdfs.page_number,\n        pdfs.pdf_file,\n        images.image_file,\n        images.image_vector,\n        pdfs.pdf_text,\n    from\n        pdf_pages as pdfs\n    join\n        output_vector_table as images\n        on\n        pdfs.paged_file_name = images.paged_file_name\n),\nfirst_10_pages as (\n  select\n    original_file_name,\n    page_number,\n    pdf_text,\n    row_number() over (partition by original_file_name order by page_number) as row_num_start,\n    row_number() over (partition by original_file_name order by page_number desc) as row_num_end\n  from\n    pdf_images_joined\n),\nlimited_pages as (\n  select\n    original_file_name,\n    page_number,\n    pdf_text\n  from \n    first_10_pages\n  where \n    row_num_start <= 10\n    or\n    row_num_end <= 2\n),\ndocument_text as (\n  select\n    original_file_name,\n    listagg(pdf_text, '\\n\\n') within group (order by page_number) as full_text\n  from \n    limited_pages\n  group by \n    original_file_name\n),\nget_document_summary as (\n    select\n      original_file_name,\n      full_text,\n      ai_complete(\n        model => 'claude-4-sonnet',\n        prompt => concat(\n          'You are analyzing the 2023 Investment Company Institute (ICI) Fact Book, the definitive statistical compendium of the US investment company industry. ',\n          'This document contains authoritative data on mutual funds, ETFs, closed-end funds, and other registered investment companies.\\n\\n',\n          \n          'CRITICAL ICI FACT BOOK CONTEXT:\\n',\n          '- Total industry assets: ~$27+ trillion across all investment company types\\n',\n          '- Time coverage: Multi-year trend data typically spanning 10+ years ending 2023\\n',\n          '- Geographic scope: Primarily US registered investment companies with some global context\\n',\n          '- Data authority: Official industry statistics used by regulators, researchers, and investment professionals\\n\\n',\n          \n          'EXTRACTION FOCUS - Identify these key document characteristics:\\n',\n          '1. ASSET UNIVERSE: What types of investment vehicles are covered (mutual funds, ETFs, closed-end, etc.)\\n',\n          '2. DATA SCOPE: Geographic coverage (US domestic, international, global)\\n',\n          '3. TIME RANGE: Years covered for trend analysis and current statistics\\n',\n          '4. STATISTICAL CATEGORIES: Asset classes, fund types, market segments analyzed\\n',\n          '5. REGULATORY CONTEXT: SEC regulations, industry standards, compliance frameworks\\n',\n          '6. MARKET ANALYSIS: Flow data, performance metrics, expense ratios, market concentration\\n\\n',\n          \n          'PRECISION REQUIREMENTS:\\n',\n          '- Use exact ICI terminology (e.g., \"registered investment companies\", \"net assets\", \"total net flows\")\\n',\n          '- Specify data years and time periods precisely\\n',\n          '- Distinguish between asset classes vs. fund types vs. investment objectives\\n',\n          '- Note geographic scope explicitly (US vs. worldwide data)\\n',\n          '- Identify statistical methodologies and data sources\\n\\n',\n          \n          'OUTPUT FORMAT - Extract only these fields if confidently identified:\\n',\n          array_to_string(\n              array_construct(\n                  'document_type',\n                  'publication_year', \n                  'primary_data_years_covered',\n                  'geographic_scope',\n                  'investment_company_types_included',\n                  'asset_classes_analyzed', \n                  'key_statistical_measures',\n                  'regulatory_framework_context',\n                  'industry_trend_timeframes',\n                  'data_source_authority'\n              ),\n              '\\n\\t* '\n          ),\n          '\\n\\nRules:\\n',\n          '1. Use ICI-standard terminology and precise financial language\\n',\n          '2. Be specific about time periods (e.g., \"2014-2023 trend analysis\")\\n',\n          '3. Distinguish between different types of financial metrics\\n',\n          '4. Only include information with high confidence\\n',\n          '5. Return ONLY key:value pairs, no additional text\\n\\n',\n          'Document content:\\n', full_text, '\\n\\n'\n        ),\n        model_parameters => {\n          'temperature': 0.1,\n          'max_tokens': 1500\n        }\n      )::string as document_metadata\n    from \n        document_text\n),\ndescribe_pages as (\n    select\n        pdf_file_name,\n        image_file_name,\n        original_file_name,\n        page_number,\n        pdf_file,\n        image_file,\n        image_vector,\n        pdf_text,\n        ai_complete(\n          model => 'claude-4-sonnet',\n          prompt => concat(\n            'You are analyzing individual pages from the 2023 ICI Investment Company Fact Book. Each page contains specific financial data, statistics, charts, or analysis segments that serve distinct research and analytical purposes.\\n\\n',\n            \n            'ICI PAGE ANALYSIS EXPERTISE:\\n',\n            '- Pages typically focus on specific data themes: asset allocation, fund flows, expense analysis, performance metrics, market trends\\n',\n            '- Statistical tables show precise numerical data with time series\\n',\n            '- Charts visualize trends, comparisons, and distributions\\n',\n            '- Text sections provide context, methodology, and interpretation\\n',\n            '- Footnotes contain critical definitional and methodological information\\n\\n',\n            \n            'PAGE-LEVEL EXTRACTION FOCUS:\\n',\n            '1. PRIMARY DATA THEME: What specific aspect of investment company data is the main focus?\\n',\n            '2. FINANCIAL METRICS: Exact types of measurements (assets, flows, returns, ratios, percentages)\\n',\n            '3. TIME DIMENSION: Specific years, quarters, or time periods covered on this page\\n',\n            '4. MARKET SEGMENTATION: Fund types, asset classes, geographic regions, or investor categories\\n',\n            '5. VISUAL ELEMENTS: Types of charts, tables, or data presentations\\n',\n            '6. QUANTITATIVE SCOPE: Scale of data (billions, trillions, percentages, basis points)\\n\\n',\n            \n            'ICI-SPECIFIC PATTERN RECOGNITION:\\n',\n            '- Asset allocation pages: Equity, fixed income, money market, hybrid breakdowns\\n',\n            '- Flow analysis pages: Net flows, inflows, outflows by fund type or time period\\n',\n            '- Performance pages: Returns, volatility, benchmarking data\\n',\n            '- Market structure pages: Concentration, market share, competitive dynamics\\n',\n            '- Expense analysis pages: Fee structures, expense ratios, cost trends\\n',\n            '- Demographic pages: Investor characteristics, distribution channels\\n\\n',\n            \n            'PRECISION REQUIREMENTS:\\n',\n            '- Identify specific ICI data categories and subcategories\\n',\n            '- Note exact time periods referenced (year-end vs. quarterly vs. cumulative)\\n',\n            '- Distinguish between gross and net measures, flows vs. assets vs. returns\\n',\n            '- Specify if data is US-only, international, or global\\n',\n            '- Identify footnotes or methodological qualifiers\\n\\n',\n            \n            'OUTPUT FIELDS - Extract only if clearly present:\\n',\n            array_to_string(\n              array_construct(\n                'primary_data_focus',\n                'specific_financial_metrics',\n                'time_periods_covered', \n                'fund_types_or_asset_classes',\n                'geographic_scope_if_specified',\n                'visual_presentation_type',\n                'key_quantitative_highlights',\n                'methodological_notes_if_present'\n              ),\n              '\\n\\t* '\n            ), '\\n\\n',\n            \n            'Rules:\\n',\n            '1. Use precise ICI terminology and financial language\\n',\n            '2. Be specific about data categories and time periods\\n',\n            '3. Focus on what makes this page unique within the larger document\\n',\n            '4. Note visual elements that would aid in retrieval\\n',\n            '5. Return ONLY key:value pairs\\n\\n',\n            'Page content:\\n', pdf_text, '\\n\\n'\n          ),\n          model_parameters => {\n            'temperature': 0.05,\n            'max_tokens': 1200\n          }\n        )::string as page_metadata\n    from\n        pdf_images_joined\n),\npages_with_metadata as (\n  select\n    page.pdf_file_name,\n    page.image_file_name,\n    page.original_file_name,\n    page.page_number,\n    page.pdf_file,\n    page.image_file,\n    page.image_vector,\n    page.pdf_text,\n    page.page_metadata,\n    doc.document_metadata\n  from\n    describe_pages page\n  join\n    get_document_summary doc\n    on \n        page.original_file_name = doc.original_file_name\n),\nsplit_pages_into_chunks as (\n    select\n        pdf_file_name,\n        image_file_name,\n        original_file_name,\n        page_number,\n        image_vector,\n        pdf_text,\n        document_metadata,\n        page_metadata,\n        ai_complete(\n            model => 'claude-4-sonnet',\n            predicate => concat(\n                'You are performing multimodal analysis of ICI Investment Company Fact Book content, analyzing both visual page images and extracted text chunks to create enriched context for financial data retrieval.\\n\\n',\n                \n                'MULTIMODAL ANALYSIS OBJECTIVE:\\n',\n                'Describe how this specific text chunk relates to the visual elements (tables, charts, graphs) and overall financial data presentation in the page image, creating searchable context for investment industry professionals.\\n\\n',\n                \n                'ICI FACT BOOK VISUAL-TEXT INTEGRATION PATTERNS:\\n\\n',\n                \n                'FOR STATISTICAL TABLES:\\n',\n                '- Text chunk position: header row, data row, footnote, or summary section\\n',\n                '- Data hierarchy: main category, subcategory, or detailed breakdown\\n',\n                '- Temporal context: specific year, time series position, or trend indicator\\n',\n                '- Cross-references: table numbers, figure citations, or related data points\\n',\n                '- Quantitative context: units (billions, percentages), scale factors, precision levels\\n\\n',\n                \n                'FOR CHARTS AND GRAPHS:\\n',\n                '- Visual relationship: axis label, data series, legend item, or chart title\\n',\n                '- Data representation: trend line point, category segment, or comparative element\\n',\n                '- Time series position: starting point, endpoint, peak/trough, or inflection point\\n',\n                '- Category classification: fund type, asset class, geographic region, or market segment\\n',\n                '- Performance indicators: growth rates, market share changes, or volatility measures\\n\\n',\n                \n                'FOR NARRATIVE AND ANALYSIS:\\n',\n                '- Data interpretation: statistical finding explanation, trend analysis, or market insight\\n',\n                '- Methodology context: calculation method, data source, or measurement standard\\n',\n                '- Industry context: regulatory impact, market dynamic, or competitive factor\\n',\n                '- Forward-looking elements: projections, implications, or industry outlook\\n',\n                '- Comparative analysis: benchmarking, historical context, or peer comparisons\\n\\n',\n                \n                'ENHANCED CONTEXT EXTRACTION:\\n',\n                '1. QUANTITATIVE PRECISION: Extract exact figures, percentages, time periods with proper units and context\\n',\n                '2. VISUAL POSITIONING: Describe where this chunk appears in charts, tables, or visual hierarchies\\n',\n                '3. ICI TERMINOLOGY: Use standard investment industry language and ICI-specific categorizations\\n',\n                '4. SEARCHABILITY: Include keywords that financial professionals would use to find this data\\n',\n                '5. CROSS-REFERENCES: Note connections to other data points, charts, or analytical sections\\n\\n',\n                \n                'FINANCIAL DATA CONTEXTUALIZATION:\\n',\n                '- Asset allocation context: Which asset classes, geographic regions, or fund types\\n',\n                '- Flow analysis context: Inflows vs. outflows, net flows, seasonal patterns\\n',\n                '- Performance context: Returns, volatility, risk-adjusted measures, benchmarking\\n',\n                '- Market structure context: Concentration, market share, competitive dynamics\\n',\n                '- Cost analysis context: Expense ratios, fee structures, cost trends over time\\n',\n                '- Regulatory context: Compliance requirements, reporting standards, rule impacts\\n\\n',\n                \n                'OUTPUT FORMAT:\\n',\n                '**Visual Context**: [How this text relates to charts, tables, or visual elements]\\n',\n                '**Data Classification**: [ICI category, fund type, asset class, or market segment]\\n',\n                '**Quantitative Details**: [Specific figures, time periods, units, and scale]\\n',\n                '**Search Keywords**: [Terms financial professionals would use to find this data]\\n',\n                '**Cross-References**: [Related data points, charts, or sections]\\n',\n                '**Industry Relevance**: [Why this data matters for investment analysis or research]\\n\\n',\n                \n                'PRECISION REQUIREMENTS:\\n',\n                '- Use exact ICI terminology and standard industry language\\n',\n                '- Maintain quantitative precision with proper units and time qualifiers\\n',\n                '- Create multiple pathways for data discovery (fund type, asset class, time period, etc.)\\n',\n                '- Link visual elements to searchable concepts\\n',\n                '- Focus on actionable insights for financial analysis\\n\\n',\n                \n                'Text chunk to analyze:\\n', value::string, '\\n\\n'\n            ),\n            file => image_file,\n            model_parameters => {\n              'temperature': 0.1,\n              'max_tokens': 1500\n            }\n        )::string as chunk_context,\n        concat(\n            '**Source File:** ', original_file_name, '\\n',\n            '**Document Metadata:**\\n', coalesce(document_metadata, 'N/A'), '\\n\\n',\n            '===========================================\\n\\n',\n            '**Page Metadata:**\\n', coalesce(page_metadata, 'N/A'), '\\n',\n            '**Page Number:** ', page_number::string, '\\n\\n',\n            '===========================================\\n\\n',\n            '**Multimodal Chunk Analysis:**\\n', chunk_context,\n             '\\n\\n===========================================\\n\\n',\n            '**Original Text Chunk:**\\n\\n', value::string\n        ) as enriched_chunk,\n        -- NEW: Store raw chunk text for hybrid search capability\n        value::string as raw_chunk_text\n    from\n        pages_with_metadata,\n    lateral flatten(\n        input=>snowflake.cortex.split_text_recursive_character(\n            pdf_text,\n            'markdown',\n            1800,\n            200\n        )\n    )\n)\nselect\n    pdf_file_name,\n    image_file_name,\n    original_file_name,\n    page_number,\n    image_vector,\n    pdf_text,\n    enriched_chunk,\n    raw_chunk_text\nfrom\n    split_pages_into_chunks\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1122a28-4e18-4f21-a398-cc026236a8b5",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "select * from pdf_images_joined;\n\nupdate  pdf_images_joined\n set image_file_name='PARSED/'||IMAGE_FILE_NAME;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6bc27bae-b050-45c3-9b60-b6271bc4971a",
   "metadata": {
    "name": "MD_cortex_search",
    "collapsed": false
   },
   "source": "## üïµÔ∏è‚Äç‚ôÄÔ∏è Build Cortex Search Service\n\nWe're using the [User-Provided Vector Embeddings in Cortex Search](https://docs.snowflake.com/LIMITEDACCESS/cortex-search/user-provided-vectors) private preview. This allows us to provide precomputed vector embeddings to index and query with Cortex Search, which allows us to use our image embeddings as part of the search service."
  },
  {
   "cell_type": "code",
   "id": "8f79e763-5106-42a8-b138-f092391a69de",
   "metadata": {
    "language": "sql",
    "name": "SQL_cortex_search"
   },
   "outputs": [],
   "source": "\n\n-- ENHANCED HYBRID SEARCH SERVICE - Indexes both raw and enriched text\ncreate or replace cortex search service docs_search_service\n    text indexes (pdf_text,enriched_chunk,raw_chunk_text)\n    vector indexes (image_vector)\n    warehouse = CORTEX_SEARCH_TUTORIAL_WH\n    target_lag = '1 day'\n    as \n    select \n        pdf_file_name,\n        image_file_name,\n        original_file_name,\n        page_number,\n        image_vector,\n        pdf_text::varchar as pdf_text,\n        enriched_chunk,\n        raw_chunk_text\n    from \n       pdf_images_joined\n;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c1630e0-2ed4-493d-869a-0e719b7fa2a7",
   "metadata": {
    "name": "MD_perform_rag",
    "collapsed": false
   },
   "source": "## ü§ñ Multimodal Document Question Answering with Image-Aided Semantic Search\n\nThis system answers precise technical questions about door hardware products using both **text** and **image-based** reasoning across PDFs. It completes a three-step multimodal process:\n\n1. **Text-Based Answering** using enriched document chunks.\n2. **Image-Based Answer Validation** via OCR page images.\n3. **Answer Synthesis** to revise and finalize the response.\n\nThis is done using the following steps:\n\n### 1. **Convert User Question into Image Embedding**\n\n* `get_text_embedding_via_image()`:\n\n  * Renders the user's question as a temporary PNG image.\n  * Uploads it to a Snowflake stage (`@utils.ai.stock_ikb_documents/queries/`).\n  * Uses `snowflake.cortex.embed_image_1024()` to generate a **multimodal embedding** via `voyage-multimodal-3`.\n\n---\n\n### 2. **Perform Semantic Search with Text + Embedding**\n\n* `query_search_service()`:\n\n  * Submits both the raw question and the image embedding to a **Cortex search service**.\n  * Returns the top 50 `ENRICHED_CHUNK`s (text + metadata + image references) relevant to the question.\n\n---\n\n### 3. **Rephrase Question for Search Compatibility**\n\n* `rephrase_for_search()`:\n\n  * Normalizes the user query by trimming and lowercasing it.\n  * Helps align question formatting with indexed content.\n\n---\n\n### 4. **Build Context and Generate Text-Based Answer**\n\n* `ai_complete_on_text()`:\n\n  * Constructs a markdown-rich prompt from the top retrieved chunks.\n  * Includes clickable links to the original PDF pages via `presigned_url`.\n  * Uses `claude-3-7-sonnet` to answer the question directly and include:\n\n    * A clear **direct answer**,\n    * A **confidence score** (0‚Äì1),\n    * A short **justification**, and\n    * Properly formatted **CITED SOURCES**:\n      [`Document Name - page X`](presigned_url)\n\n---\n\n### 5. **Deduplicate and Filter Pages for Image Critique**\n\n* Duplicates are removed based on `(PDF_FILE_NAME, IMAGE_FILE_NAME)` pairs.\n* `extract_cited_docs_and_pages()` ensures only **relevant document pages** are processed.\n* `extract_page_number()` helps match images to cited pages.\n\n---\n\n### 6. **Run Image-Based Validations (Async)**\n\n* `ai_complete_on_image_async()`:\n\n  * Submits each matched image to `claude-3-7-sonnet` via `ai_complete()` using:\n\n    * Document metadata\n    * Page number\n    * The original text answer for critique\n  * Prompt instructs the model to confirm or revise the answer, ensuring it's grounded in the visual page content.\n\n* `resolve_async_job()`:\n\n  * Polls the result and extracts fields like:\n\n    * `RESULT`, `PAGE_NUMBER`, `IMAGE_FILE_NAME`, `PRESIGNED_URL`.\n\n---\n\n### 7. **Filter Image-Based Answers by Confidence**\n\n* `filter_by_confidence()`:\n\n  * Retains only image completions with `CONFIDENCE >= 0.5`.\n  * Helps ensure only high-quality critiques contribute to the final answer.\n\n---\n\n### 8. **Synthesize the Final Answer for the User**\n\n* `synthesise_all_answers()`:\n\n  * Merges text and image critiques into a unified prompt.\n  * Final LLM prompt includes:\n\n    * Original text result\n    * All image critiques (linked to page images)\n  * The LLM is instructed to:\n\n    * Revise or reaffirm the answer\n    * Rephrase for user clarity (customer-facing tone)\n    * Exclude technical fields like `CONFIDENCE` or `JUSTIFICATION`\n    * Append a **\"Cited Sources\"** section with markdown hyperlinks\n\n---"
  },
  {
   "cell_type": "code",
   "id": "ac3a442c-1d03-4045-8314-570caf933df2",
   "metadata": {
    "language": "python",
    "name": "PY_perform_rag"
   },
   "outputs": [],
   "source": "# def query_search_service(session, my_service, query_text):\n#     query_embedding = get_text_embedding_via_image(session, query_text)\n#     resp = my_service.search(\n#         query = query_text,\n#         experimental = {\n#             \"QueryEmbedding\": query_embedding\n#         },\n#         columns=[\n#             \"ENRICHED_CHUNK\",\n#             \"PDF_FILE_NAME\",\n#             \"IMAGE_FILE_NAME\",\n#             \"ORIGINAL_FILE_NAME\",\n#             \"PAGE_NUMBER\"\n#         ],\n#         limit=5\n#     )\n#     return resp.to_json()\n \n\ndef query_multi_index_search_service(session, my_service, query_text):\n    \"\"\"ENHANCED HYBRID SEARCH: Image + Enriched Text + Raw Text\"\"\"\n    query_embedding = get_text_embedding_via_image(session, query_text)\n    \n    resp = my_service.search(\n        # Use ONLY multi_index_query, not both query and multi_index_query\n        multi_index_query={\n            \"image_vector\": [\n                {\"vector\": query_embedding}],\n             \"enriched_chunk\":[{\"text\":query_text}],\n            \"pdf_text\":[{\"text\":query_text}],\n            \"raw_chunk_text\":[{\"text\":query_text}]},\n        \n        columns=[\n            \"ENRICHED_CHUNK\",\n            \"RAW_CHUNK_TEXT\",\n            \"PDF_FILE_NAME\",\n            \"IMAGE_FILE_NAME\",\n            \"ORIGINAL_FILE_NAME\",\n            \"PAGE_NUMBER\"\n        ],\n        limit=1000\n    )\n    \n    return resp.to_json() \n\n# def query_multi_index_search_service(session, my_service, query_text):\n#     query_embedding = get_text_embedding_via_image(session, query_text)\n    \n#     resp = my_service.search(\n#         # Use ONLY multi_index_query, not both query and multi_index_query\n#         multi_index_query={\n#             \"image_vector\": [\n#                 {\"vector\": query_embedding}\n#             ],\n#         },\n        \n#         columns=[\n#             \"ENRICHED_CHUNK\",\n#             \"PDF_FILE_NAME\",\n#             \"IMAGE_FILE_NAME\",\n#             \"ORIGINAL_FILE_NAME\",\n#             \"PAGE_NUMBER\"\n#         ],\n#         limit=20\n#     )\n    \n#     return resp.to_json()\n\ndef create_temp_image_from_text(text: str) -> tuple[str, str]:\n    query_hash = hashlib.md5(text.strip().lower().encode()).hexdigest()\n    image_filename = f\"{query_hash}.png\"\n\n    temp_file = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n    file_path = temp_file.name\n    temp_file.close()\n\n    image = Image.new(\"RGB\", (1000, 200), \"white\")\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.load_default()\n    draw.text((10, 10), text, fill=\"black\", font=font)\n    image.save(file_path)\n\n\n    return file_path, image_filename\n\n\ndef extract_cited_docs_and_pages(text_answer_str):\n    cited = {}\n    \n    # Find the CITED SOURCES section (more robust extraction)\n    cited_section_match = re.search(r\"CITED SOURCES:\\s*(.+?)(?:\\n\\n|$)\", text_answer_str, re.IGNORECASE | re.DOTALL)\n    \n    if cited_section_match:\n        cited_section = cited_section_match.group(1)\n        print(f\"DEBUG: Found cited section: {cited_section[:200]}...\")\n        \n        # Pattern 1: [document - page X](url) format\n        doc_page_pairs = re.findall(r\"\\[([a-zA-Z0-9._ -]+?)\\s*-\\s*page\\s*(\\d+)\\]\", cited_section)\n        for doc, page in doc_page_pairs:\n            doc = doc.strip().lower()\n            page = page.strip()\n            cited.setdefault(doc, set()).add(page)\n            print(f\"DEBUG: Added citation: {doc} -> page {page}\")\n        \n        # Pattern 2: [document](url) format - fallback for documents without explicit pages\n        if not doc_page_pairs:\n            doc_links = re.findall(r\"\\[([a-zA-Z0-9._ -]+?)\\]\\(\", cited_section)\n            for doc in doc_links:\n                doc_clean = doc.strip().lower()\n                cited.setdefault(doc_clean, set()).add(\"*\")  # Wildcard for any page\n                print(f\"DEBUG: Added wildcard citation: {doc_clean} -> *\")\n    else:\n        print(\"DEBUG: No CITED SOURCES section found in text\")\n    \n    print(f\"DEBUG: Final extracted citations: {cited}\")\n    return cited\n\ndef extract_page_number(image_file_name: str) -> str:\n    match = re.search(r'_page_(\\d+)\\.png$', image_file_name)\n    return match.group(1) if match else \"N/A\"\n\n\ndef file_exists_in_stage(session, stage_name: str, file_path: str) -> bool:\n    result = session.sql(f\"list @{stage_name}/{file_path}\").collect()\n    return bool(result)\n\n\ndef fuzzy_match(a, b, threshold=0.6):\n    return SequenceMatcher(None, a.lower(), b.lower()).ratio() >= threshold\n\n\ndef upload_file_to_stage(session, local_path: str, stage_name: str, dest_file_name: str):\n    temp_dir = tempfile.gettempdir()\n    temp_named_path = os.path.join(temp_dir, dest_file_name)\n\n    os.makedirs(os.path.dirname(temp_named_path), exist_ok=True)\n    shutil.copyfile(local_path, temp_named_path)\n\n    try:\n        result = session.file.put(\n            temp_named_path,\n            f\"@{stage_name}/queries\",\n            overwrite=True,\n            auto_compress=False\n        )\n        \n    finally:\n        os.remove(temp_named_path)\n\n\ndef get_text_embedding_via_image(\n    session, \n    text: str, \n    stage_name=\"@cortex_search_tutorial_db.public.doc_repo\"\n):\n    temp_path, image_filename = create_temp_image_from_text(text)\n    stage_subpath = f\"queries/{image_filename}\"\n\n    try:\n        if not file_exists_in_stage(session, stage_name.lstrip(\"@\"), stage_subpath):\n            upload_file_to_stage(session, temp_path, stage_name.lstrip(\"@\"), stage_subpath)\n            \n        query = f\"\"\"\n            select \n                AI_EMBED(\n                    'voyage-multimodal-3', \n                    '{stage_name}+{stage_subpath.lstrip('/')}'\n                )\n        \"\"\"\n        embedding = session.sql(query).collect()[0][0]\n    finally:\n        os.remove(temp_path)\n\n    return embedding\n\n\ndef resolve_async_job(job):\n    try:\n        row = job.result()[0].asDict()\n        return {\n            \"RESULT\": row[\"RESULT\"],\n            \"ORIGINAL_FILE_NAME\": row[\"ORIGINAL_FILE_NAME\"],\n            \"IMAGE_FILE_NAME\": row[\"IMAGE_FILE_NAME\"],\n            \"PRESIGNED_URL\": row.get(\"PRESIGNED_URL\", \"#\")\n        }\n    except Exception as e:\n        return {\n            \"RESULT\": f\"Error: {e}\",\n            \"ORIGINAL_FILE_NAME\": None,\n            \"IMAGE_FILE_NAME\": None,\n            \"PRESIGNED_URL\": \"#\"\n        }\n\n\ndef rephrase_for_search(question):\n    return question.strip().lower()\n\n\ndef filter_by_confidence(responses, threshold=0.5):\n    filtered = []\n    for item in responses:\n        match = re.search(r\"CONFIDENCE:\\s*([0-9.]+)\", item[\"RESULT\"], re.IGNORECASE)\n        score = float(match.group(1)) if match else 0.0\n        if score >= threshold:\n            filtered.append(item)\n    return filtered\n\n\ndef sql_escape(value):\n    return str(value).replace(\"'\", \"''\") if value is not None else \"\"\n\n\ndef run_model(model_name, llm_prompt, session, temperature, max_tokens, top_p, guardrails, stream):\n    return complete(\n        model=model_name,\n        prompt=[{\"role\": \"user\", \"content\": llm_prompt}],\n        session=session,\n        options=CompleteOptions(\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            guardrails=guardrails\n        ),\n        stream=stream\n    )\n\n\ndef ai_complete_on_text(session, question, retrieved_chunks):\n    seen = set()\n    enriched_context_blocks = []\n\n    for chunk in retrieved_chunks:\n        enriched_chunk = chunk[\"ENRICHED_CHUNK\"]\n        original_file = chunk.get(\"ORIGINAL_FILE_NAME\")\n        image_file = chunk.get(\"IMAGE_FILE_NAME\")\n\n        if not image_file or not original_file:\n            continue\n\n        key = (original_file, image_file, enriched_chunk)\n        if key in seen:\n            continue\n        seen.add(key)\n\n        # Generate presigned URL\n        presigned_url = session.sql(\n            f\"SELECT GET_PRESIGNED_URL(@cortex_search_tutorial_db.public.doc_repo, '{sql_escape(image_file)}')\"\n        ).collect()[0][0]\n\n        # Format for the model\n        block = dedent(f\"\"\"\n        ---\n        üìÑ **Source**: [{original_file}]({presigned_url})\n        üìú **Extracted Content**:\n        {enriched_chunk}\n        \"\"\").strip()\n\n        enriched_context_blocks.append(block)\n\n    if not enriched_context_blocks:\n        return {\"result\": \"No usable context.\", \"metadata\": {}}\n\n    full_context = \"\\n\\n\".join(enriched_context_blocks)\n\n    prompt = dedent(f\"\"\"\n    You are an expert analyst of the 2023 Investment Company Institute (ICI) Fact Book, a comprehensive \n    statistical compendium containing precise financial data about US and global investment companies.\n\n    ## 2023 ICI FACT BOOK SPECIFIC INTELLIGENCE:\n\n    ### Known Data Context:\n    - Total registered investment company assets: ~$27+ trillion as of year-end 2023\n    - Breakdown by: Mutual Funds (~$20+ trillion), ETFs (~$6+ trillion), Closed-End (~$200+ billion)\n    - Major asset classes: Equity (domestic/international), Fixed Income, Money Market, Hybrid\n    - Key trend timeframes: 2019-2023 (5-year), 2014-2023 (10-year)\n\n    ### Critical Terminology Precision:\n    - \"Net assets\" = Assets minus liabilities (standard ICI metric)\n    - \"Total net assets\" = Sum across all fund types unless qualified\n    - \"Asset allocation\" = Investment portfolio composition by asset class\n    - \"Fund assets\" = Assets within specific fund type only\n    - \"Investment company assets\" = All registered funds combined\n\n    ## ICI FACT BOOK DATA ARCHITECTURE:\n    \n    ### Asset Classification Hierarchy:\n    - **LEVEL 1 - Asset Classes**: Equity, Fixed Income, Money Market, Hybrid/Balanced\n    - **LEVEL 2 - Geographic Scope**: Domestic, International, Global, Regional\n    - **LEVEL 3 - Investment Vehicles**: Mutual Funds, ETFs, Closed-End Funds\n    - **LEVEL 4 - Investment Objectives**: Growth, Value, Blend, Sector-Specific, Target-Date\n    \n    ### Data Presentation Standards:\n    - **Net Assets**: Always in billions of dollars unless specified otherwise\n    - **Percentages**: Typically represent share of total within category\n    - **Time Periods**: Year-end data (December 31) unless noted as quarterly\n    - **Geographic Coverage**: US data unless explicitly marked as \"Worldwide\"\n    - **Fund Universe**: All registered investment companies unless subset specified\n    \n    ### Visual Data Types in Context:\n    - **Figure Tables**: Numerical data in structured rows/columns with precise values\n    - **Bar Charts**: Year-over-year comparisons, often 5-10 year timeframes  \n    - **Pie Charts**: Percentage breakdowns that sum to 100%\n    - **Line Graphs**: Trend analysis over multiple years\n    - **Flow Charts**: Net flows (inflows minus outflows) in billions\n\n    ## PRECISION GUARDRAILS:\n\n    ### Red Flag Validation Checks:\n    1. **Scale Reasonableness**: US mutual fund assets should be $15-25 trillion range\n    2. **Percentage Validation**: Asset allocation percentages must sum to ~100%\n    3. **Temporal Consistency**: 2023 data should show logical progression from 2022\n    4. **Geographic Logic**: US domestic equity typically 40-60% of total equity assets\n    5. **Fund Type Ratios**: Mutual funds typically 3-4x larger than ETF assets\n\n    ## CRITICAL INTERPRETATION RULES:\n    \n    ### Asset Allocation Questions:\n    - \"Net investments by asset class\" = TOP-LEVEL asset allocation across equity/fixed income/money market/hybrid\n    - \"Total net assets\" = Sum across ALL investment company types (mutual funds + ETFs + closed-end)\n    - \"Asset allocation\" WITHOUT qualifiers = Comprehensive breakdown across all major categories\n    - \"Fund assets\" WITH qualifiers = Specific to mentioned fund type only\n    \n    ### Temporal Context:\n    - Always specify data year (2023, 2022, etc.)\n    - Note if data is year-end vs. quarterly vs. cumulative\n    - Multi-year questions require trend analysis across time periods\n    \n    ### Scale and Scope Precision:\n    - Billions vs. trillions notation matters\n    - US-only vs. global data distinction is critical\n    - Registered vs. unregistered investment companies\n    - Retail vs. institutional share classes\n\n    ## INTELLIGENT QUESTION ROUTING:\n\n    ### Question Pattern Analysis:\n    - **Allocation Questions** (\"by asset class\", \"breakdown\", \"distribution\")\n      ‚Üí Expect percentage outputs from comprehensive data tables\n    - **Trend Questions** (\"growth\", \"change\", \"over time\")  \n      ‚Üí Expect directional analysis from time series data\n    - **Comparison Questions** (\"vs\", \"compared to\", \"relative\")\n      ‚Üí Expect relative metrics from comparative charts\n    - **Scale Questions** (\"total\", \"size\", \"how much\")\n      ‚Üí Expect absolute values from aggregate statistics\n\n    ## ENHANCED ACCURACY PROTOCOLS:\n    \n    ### Data Validation Checklist:\n    1. **Scope Match**: Does data scope exactly match question parameters?\n    2. **Time Alignment**: Is the time period precisely what was asked?\n    3. **Scale Verification**: Are units (billions/percentages) correctly interpreted?\n    4. **Completeness Check**: For \"total\" questions, is all relevant data included?\n    5. **Category Precision**: Are asset classes vs. fund types vs. objectives correctly distinguished?\n    \n    ### Common Precision Errors to Avoid:\n    - Confusing \"mutual fund equity assets\" with \"total equity assets across all vehicles\"\n    - Mixing domestic and international data when only one was requested\n    - Using partial year data when year-end was implied\n    - Conflating investment objectives with asset classes\n    - Missing geographic or vehicle-type qualifiers\n\n    ## CONFIDENCE SCORING PRECISION:\n\n    ### Confidence Level Guidelines:\n    - **1.0**: Direct table lookup with exact match to question parameters\n    - **0.9**: Clear chart data with minor interpolation required\n    - **0.8**: Multiple consistent sources supporting same conclusion\n    - **0.7**: Single good source but some scope mismatch (e.g., 2022 vs 2023 data)\n    - **0.6**: Partial data requiring reasonable inference\n    - **0.5**: Limited data with significant uncertainty\n    - **<0.5**: Insufficient data to answer question reliably\n\n    ### Confidence Reduction Triggers:\n    - Data from different time periods than requested (-0.1 to -0.2)\n    - Geographic scope mismatch (US vs global) (-0.2)\n    - Fund type scope mismatch (specific vs total) (-0.1 to -0.3)\n    - Conflicting data between sources (-0.3 to -0.5)\n    \n    ---\n    \n    ## QUESTION ANALYSIS:\n    **User Question**: {question.strip()}\n    \n    **Question Type Identification**:\n    - Asset allocation breakdown? Geographic analysis? Fund flow trends? Performance comparison?\n    - Time-specific or trend analysis? Single category or comprehensive view?\n    - Absolute values or relative percentages? Current state or historical change?\n    \n    ## CONTEXT BLOCKS:\n    {full_context}\n    \n    ---\n    \n    ## REQUIRED OUTPUT FORMAT:\n    \n    DIRECT ANSWER: [Precise numerical answer with units, time period, and scope clearly specified]\n    \n    CONFIDENCE: [0.0-1.0 following guidelines above, with specific reasoning for score]\n    \n    JUSTIFICATION: [Explanation referencing specific ICI data points, noting any limitations, scope restrictions, or validation checks applied]\n    \n    CITED SOURCES: [2023-factbook - page X](presigned_url) [2023-factbook - page Y](presigned_url)\n    \n    ## CRITICAL SUCCESS METRICS:\n    - Numerical precision to appropriate decimal places\n    - Explicit time period and geographic scope\n    - Clear distinction between asset classes, fund types, and investment objectives  \n    - Comprehensive coverage when \"total\" or \"all\" is requested\n    - Acknowledgment of data limitations or gaps if present\n    - Applied validation checks against known ICI data patterns\n    \n    Your Response:\n    \"\"\")\n\n    result = complete(\n        model=\"claude-4-sonnet\",\n        prompt=[{\"role\": \"user\", \"content\": prompt}],\n        session=session,\n        options=CompleteOptions(\n            temperature=0.05,  # Very low for maximum precision\n            max_tokens=1500,   # Conservative to avoid token limit issues\n            top_p=0.9,\n            guardrails=False\n        ),\n        stream=False\n    )\n\n    return {\n        \"result\": \"\".join(result),\n        \"metadata\": {\n            \"source\": \"TEXT\",\n            \"num_chunks\": len(retrieved_chunks)\n        },\n        \"prompt\": prompt\n    }\n    \n\ndef ai_complete_on_image_async(session, question, item, text_answer):\n    image_file_name = item[\"IMAGE_FILE_NAME\"]\n    original_file_name = item.get(\"ORIGINAL_FILE_NAME\", \"\")\n    page_number = item.get(\"PAGE_NUMBER\", \"\")\n\n    # Escape for SQL\n    image_file_escaped = sql_escape(image_file_name)\n    original_file_escaped = sql_escape(original_file_name)\n    document_metadata_escaped = sql_escape(original_file_name)\n    page_metadata_escaped = sql_escape(str(page_number))\n    answer_snippet_escaped = sql_escape(text_answer[\"result\"][:2000])\n\n    prompt = dedent(f\"\"\"\n    You are an expert visual analyst specializing in ICI Investment Company Fact Book financial charts, \n    tables, and infographics. Your role is to extract precise data from visual elements and validate \n    text-based answers against actual document imagery.\n\n    ## ICI VISUAL DATA EXPERTISE:\n\n    ### Chart Type Recognition & Analysis:\n    - **Statistical Tables**: Multi-column layouts with headers, often showing year-over-year data\n      ‚Üí Extract: Exact values, time periods, row/column labels, footnotes\n    - **Horizontal Bar Charts**: Category comparisons or time series\n      ‚Üí Extract: Scale values, category labels, time periods, data values\n    - **Pie Charts**: Percentage breakdowns of total allocation\n      ‚Üí Extract: Segment percentages, labels, total represented, time period\n    - **Line Graphs**: Trend analysis over multiple years\n      ‚Üí Extract: Axis labels, scale, trend direction, specific data points\n    - **Infographics**: Key statistics with visual emphasis\n      ‚Üí Extract: Highlighted numbers, comparative ratios, summary statistics\n\n    ### ICI-Specific Visual Patterns:\n    - **Asset Allocation Pies**: Typically show equity/fixed income/money market/hybrid splits\n    - **Flow Charts**: Show net flows with positive/negative indicators, usually in billions\n    - **Time Series**: Usually 5-10 year timeframes ending in current year (2023)\n    - **Geographic Breakdowns**: US vs. International or regional distributions\n    - **Fund Type Comparisons**: Mutual funds vs. ETFs vs. closed-end funds\n\n    ### ADVANCED VISUAL INTELLIGENCE:\n\n    #### ICI Chart Pattern Recognition:\n    - **Figure Numbers**: ICI uses \"Figure X.X\" numbering - extract for precise citation\n    - **Table Headers**: Often multi-level headers (Year, Category, Subcategory)\n    - **Footnote Symbols**: *, ‚Ä†, ‚Ä° indicate important qualifiers - ALWAYS check\n    - **Color Coding**: Consistent colors for fund types across charts\n    - **Scale Breaks**: Watch for axis breaks that might distort visual interpretation\n\n    #### Visual Data Extraction Hierarchy:\n    1. **Primary Data**: Main chart/table values (highest priority)\n    2. **Footnotes**: Critical context and definitions  \n    3. **Source Lines**: Data collection methodology and timing\n    4. **Axis Labels**: Units, time periods, geographic scope\n    5. **Legend Information**: Category definitions and color coding\n\n    ### Visual Data Extraction Protocol:\n    1. **Chart Title & Context**: What is being measured, time period, scope\n    2. **Axis Labels & Scales**: Units (billions, percentages), time periods, categories\n    3. **Data Values**: Precise numbers, percentages, trends\n    4. **Footnotes & Qualifiers**: Important context about data scope or methodology\n    5. **Visual Emphasis**: What data points are highlighted or emphasized\n\n    ## VALIDATION TASK:\n\n    **Original Question**: {question.strip()}\n    \n    **Text Answer Being Validated**:\n    {text_answer[\"result\"]}\n    \n    **Image Source**: Document: `{original_file_name}`, Page: {page_number}\n\n    ## SYSTEMATIC VISUAL ANALYSIS:\n\n    ### Step 1: Image Content Identification\n    - What type of visual element is this? (table, chart, infographic, mixed)\n    - What is the primary data being presented?\n    - What time period and scope does it cover?\n    - Are there Figure numbers or Table numbers for precise citation?\n    \n    ### Step 2: Precise Data Extraction\n    - Extract all relevant numerical values visible in the image\n    - Note units (billions, percentages, etc.), time periods, and categorical labels\n    - Identify any footnotes, symbols, or qualifiers\n    - Check for multi-level headers or complex data structures\n    \n    ### Step 3: Answer Validation\n    - Compare text answer values with visual data point by point\n    - Check time periods, scope, and units match exactly\n    - Verify completeness - is any relevant visual data missing from text answer?\n    - Assess if text answer scope aligns with visual data scope\n    \n    ### Step 4: Accuracy Assessment\n    - Are there numerical discrepancies between text and visual?\n    - Is the text answer scope too narrow or too broad for the visual data?\n    - Does the text answer properly interpret the visual context and footnotes?\n    - Are there additional insights in the visual that enhance the answer?\n\n    ## ENHANCED VALIDATION CRITERIA:\n\n    ### Numerical Precision:\n    - Values match exactly or within reasonable rounding (¬±0.1% for percentages)\n    - Units (billions/percentages/ratios) correctly interpreted\n    - Time periods precisely aligned with what's shown\n    - Scale factors (thousands, millions, billions) properly applied\n    \n    ### Scope Alignment:\n    - Geographic scope (US vs. global) correctly identified from visual labels\n    - Fund type coverage (all vs. specific) properly interpreted from chart context\n    - Asset class vs. fund type distinction maintained per visual categorization\n    - Time period coverage matches visual data timeframe\n    \n    ### Completeness Assessment:\n    - All relevant visual data incorporated into assessment\n    - No cherry-picking of convenient data points\n    - Comprehensive answer when visual shows comprehensive data\n    - Footnotes and qualifiers properly considered\n\n    ### ICI-Specific Validation:\n    - Asset allocation percentages sum to 100% (¬±1% for rounding)\n    - Fund type ratios align with known ICI patterns (MF > ETF > CEF)\n    - Time series show logical progression year-over-year\n    - Geographic splits align with US investment patterns\n\n    ## REQUIRED OUTPUT FORMAT:\n\n    CRITIQUE_RESULT: [CONFIRMED/REQUIRES_CORRECTION/NEEDS_ENHANCEMENT] - [Brief assessment with specific reasoning]\n    \n    VISUAL_DATA_EXTRACTED: [Specific values, percentages, trends visible in image with exact figures, units, and time periods]\n    \n    ACCURACY_VALIDATION: [Detailed point-by-point comparison of text answer vs. visual data with specific discrepancies noted]\n    \n    SCOPE_ASSESSMENT: [Whether text answer scope matches visual data scope - time period, geography, fund types, completeness]\n    \n    FOOTNOTE_ANALYSIS: [Any footnotes, symbols, or qualifiers visible that affect interpretation]\n    \n    MISSING_INSIGHTS: [Any relevant data visible in image but not captured in text answer]\n    \n    CORRECTED_ANSWER: [If corrections needed, provide precise corrected answer based on visual data with exact values and proper context]\n    \n    CONFIDENCE_IN_VALIDATION: [0.0-1.0 based on clarity of visual data, completeness of extraction, and certainty of assessment]\n    \n    ## CRITICAL VALIDATION FOCUS:\n    - ICI Fact Book visual elements are authoritative source of truth\n    - Extract exact numerical values, not approximations\n    - Consider footnotes and qualifiers as critical context\n    - Distinguish between individual data points and totals/summaries\n    - Maintain precision in temporal and geographic scope\n    - Apply ICI-specific knowledge of data patterns and relationships\n    \n    Analysis:\n    \"\"\")\n\n    prompt_escaped = prompt.replace(\"'\", \"\\\\'\")\n\n    df = session.sql(f\"\"\"\n        select \n            '{original_file_escaped}' as original_file_name,\n            '{image_file_escaped}' as image_file_name,\n            '{document_metadata_escaped}' as document_metadata,\n            '{page_metadata_escaped}' as page_metadata,\n            get_presigned_url('@CORTEX_SEARCH_TUTORIAL_DB.public.doc_repo', '{image_file_escaped}') as presigned_url,\n            ai_complete(\n                'claude-4-sonnet',\n                '{prompt_escaped}',\n                to_file('@CORTEX_SEARCH_TUTORIAL_DB.public.doc_repo', '{image_file_escaped}'),\n                object_construct('temperature', 0.1, 'top_p', 0.9, 'max_tokens', 2500, 'guardrails', FALSE)\n            ) as result\n    \"\"\")\n    return df.collect_nowait()\n\n\ndef synthesise_all_answers(session, question, text_answer_dict, image_answer_dicts):\n    text_result = text_answer_dict[\"result\"]\n    text_meta = text_answer_dict.get(\"metadata\", {})\n\n    image_sections = []\n    for img in image_answer_dicts:\n        presigned_link = img.get(\"PRESIGNED_URL\", \"#\")\n        section = dedent(f\"\"\"\n        --- \n        üìÑ **Source**: [{img[\"ORIGINAL_FILE_NAME\"]}]({presigned_link})\n        üñºÔ∏è Image File: `{img[\"IMAGE_FILE_NAME\"]}`\n\n        üìò Page Metadata:\n        {img.get(\"PAGE_METADATA\", \"N/A\")}\n\n        üìö Document Metadata:\n        {img.get(\"DOCUMENT_METADATA\", \"N/A\")}\n\n        üìå Visual Validation Results:\n        {img[\"RESULT\"]}\n        \"\"\")\n        image_sections.append(section.strip())\n\n    image_critique_block = \"\\n\\n\".join(image_sections)\n\n    prompt = dedent(f\"\"\"\n    You are synthesizing the definitive answer to a question about 2023 ICI Investment Company Fact Book data, \n    combining text-based analysis with visual validation from actual document pages to create the most \n    accurate and authoritative response possible.\n\n    ## SYNTHESIS OBJECTIVE:\n    Create the most accurate, complete, and precise answer by integrating:\n    1. Text-based analysis from enriched document chunks\n    2. Visual validation from actual ICI Fact Book page images\n    3. Cross-validation between multiple data sources\n    4. Application of ICI-specific knowledge and data patterns\n\n    ## USER QUESTION:\n    {question}\n\n    ## TEXT-BASED ANALYSIS:\n    {text_result}\n\n    ## VISUAL VALIDATION RESULTS:\n    {image_critique_block}\n\n    ## SYNTHESIS REQUIREMENTS:\n\n    ### Accuracy Priority Hierarchy:\n    1. **Visual data from ICI pages** (authoritative when clear and relevant)\n    2. **Text analysis validated by visuals** (high confidence)\n    3. **Consistent text analysis across sources** (good confidence)\n    4. **Single source text analysis** (moderate confidence)\n    5. **Inference from partial data** (low confidence - note limitations)\n\n    ### Conflict Resolution Protocol:\n    - If visual contradicts text: Prioritize visual data (ICI pages are source of truth)\n    - If multiple visuals conflict: Note discrepancy and use most comprehensive source\n    - If visual unclear: Rely on text analysis but note visual limitation\n    - If both uncertain: Provide best available answer with clear confidence qualifier\n\n    ### Precision Standards for ICI Data:\n    - Maintain exact numerical values from authoritative sources\n    - Specify time periods (year-end 2023, Q4 2023, etc.)\n    - Include geographic scope (US, global, international)\n    - Note data universe (all investment companies, specific fund types)\n    - Use proper ICI terminology and category definitions\n    - Include appropriate units (billions of dollars, percentages, basis points)\n\n    ### Completeness Assessment:\n    - Ensure answer fully addresses the question scope\n    - Include all relevant data when comprehensive breakdown requested\n    - Distinguish between asset classes, fund types, and investment objectives\n    - Note if partial data or if additional context would enhance understanding\n    - Address both quantitative data and qualitative insights when relevant\n\n    ## ICI-SPECIFIC SYNTHESIS INTELLIGENCE:\n\n    ### Data Validation Against Known Patterns:\n    - Total investment company assets: ~$27+ trillion (2023)\n    - Mutual funds should dominate (70-75% of total)\n    - ETFs growing but still smaller (20-25% of total)\n    - Closed-end funds smallest segment (1-2% of total)\n    - Equity typically largest asset class (50-60%)\n    - Fixed income second largest (25-35%)\n    - Money market varies with market conditions (5-15%)\n\n    ### Synthesis Quality Checks:\n    - Do totals add up appropriately?\n    - Are percentages reasonable within ICI context?\n    - Does temporal data show logical progression?\n    - Are geographic splits consistent with US investment patterns?\n    - Do fund type ratios align with market structure?\n\n    ## FINAL ANSWER REQUIREMENTS:\n\n    **Tone**: Professional, authoritative, precise (suitable for financial analysis)\n    **Structure**: \n    1. Direct answer to question with key figures\n    2. Additional context and breakdowns as relevant\n    3. Time period and scope qualifiers\n    4. Any important limitations or notes\n\n    **Precision**: \n    - Exact figures with proper units and context\n    - Appropriate decimal places (typically 1 decimal for percentages)\n    - Clear temporal and geographic qualifiers\n    - Proper distinction between categories\n\n    **Sources**: Clean citation format with working hyperlinks to actual pages\n\n    ## ENHANCED SYNTHESIS PROTOCOL:\n\n    ### Integration Strategy:\n    1. **Start with most authoritative data** (visual validation when available)\n    2. **Layer in supporting context** from text analysis\n    3. **Cross-reference for consistency** across all sources\n    4. **Apply ICI knowledge** for reasonableness checks\n    5. **Note any limitations** or gaps in available data\n\n    ### Output Optimization:\n    - Lead with direct numerical answer when possible\n    - Provide context that enhances understanding\n    - Use proper financial terminology\n    - Maintain professional tone suitable for investment industry\n    - Include actionable insights when relevant\n\n    ## OUTPUT FORMAT:\n\n    [Provide comprehensive, accurate answer based on synthesis of all available data, leading with direct response to question, followed by relevant context and qualifiers]\n\n    **Cited Sources**:\n    - [2023-factbook - page X](url)\n    - [2023-factbook - page Y](url)\n\n    ## SYNTHESIS SUCCESS CRITERIA:\n    - Numerical accuracy verified against visual sources when available\n    - Scope precisely matches question parameters\n    - Time periods and geographic context clearly specified\n    - Comprehensive coverage when broad questions asked\n    - Professional presentation suitable for financial analysis\n    - Confidence level appropriately calibrated to data quality\n    - ICI-specific patterns and knowledge properly applied\n\n    Final Answer:\n    \"\"\")\n\n    result = complete(\n        model=\"claude-4-sonnet\",\n        prompt=prompt,\n        session=session,\n        options=CompleteOptions(\n            temperature=0.1,  # Low for precise synthesis\n            max_tokens=1500,  # Conservative to avoid token limits\n            top_p=0.9,\n            guardrails=False\n        ),\n        stream=False\n    )\n\n    return \"\".join(result)\n\nimport re\n\ndef smart_chunk_selection(chunks, question, max_chunks=10):\n    \"\"\"ENHANCED HYBRID CHUNK SELECTION: Balances enriched context with raw text precision\"\"\"\n    \n    # ICI-specific high-value keywords with enhanced weighting\n    ici_keywords = {\n        'asset': 3, 'allocation': 3, 'class': 2, 'total': 3, 'net': 2,\n        'equity': 2, 'fixed': 2, 'income': 2, 'money': 2, 'market': 2,\n        'mutual': 2, 'fund': 2, 'etf': 2, 'exchange': 2, 'traded': 2,\n        'billion': 3, 'trillion': 3, 'percentage': 2, 'breakdown': 3,\n        'domestic': 2, 'international': 2, 'flow': 2, 'investment': 1,\n        'company': 1, 'registered': 2, '2023': 3, '2022': 2\n    }\n    \n    question_words = [word.lower().strip('.,!?') for word in question.split()]\n    \n    scored_chunks = []\n    for chunk in chunks:\n        # HYBRID SCORING: Consider both enriched and raw content\n        enriched_text = chunk.get(\"ENRICHED_CHUNK\", \"\").lower()\n        raw_text = chunk.get(\"RAW_CHUNK_TEXT\", \"\").lower()\n        \n        # Base relevance from question keywords in BOTH texts\n        enriched_base = sum(3 for word in question_words if len(word) > 3 and word in enriched_text)\n        raw_base = sum(4 for word in question_words if len(word) > 3 and word in raw_text)  # Higher weight for exact matches\n        \n        # ICI-specific scoring for both texts\n        enriched_ici = sum(weight for term, weight in ici_keywords.items() if term in enriched_text)\n        raw_ici = sum(weight * 1.2 for term, weight in ici_keywords.items() if term in raw_text)  # Slight boost for raw\n        \n        # Numerical data bonuses (more likely in raw text for exact figures)\n        enriched_numerical = len(re.findall(r'\\b\\d+\\.?\\d*\\b', enriched_text)) * 0.3\n        raw_numerical = len(re.findall(r'\\b\\d+\\.?\\d*\\b', raw_text)) * 0.8  # Higher weight for raw numbers\n        \n        # Percentage bonuses\n        enriched_percentage = len(re.findall(r'\\b\\d+\\.?\\d*%', enriched_text)) * 0.5\n        raw_percentage = len(re.findall(r'\\b\\d+\\.?\\d*%', raw_text)) * 1.2  # Raw percentages more precise\n        \n        # Year bonuses for both\n        enriched_year = 1 if '2023' in enriched_text else (0.5 if '2022' in enriched_text else 0)\n        raw_year = 2 if '2023' in raw_text else (1 if '2022' in raw_text else 0)\n        \n        # QUALITY BONUSES:\n        # Enriched chunks with visual context get bonus\n        visual_bonus = 1 if 'visual context' in enriched_text or 'chart' in enriched_text or 'table' in enriched_text else 0\n        \n        # Raw chunks with exact financial terms get bonus\n        financial_bonus = 1 if any(term in raw_text for term in ['$', 'billion', 'trillion', 'assets', 'net']) else 0\n        \n        # HYBRID TOTAL SCORE\n        total_score = (\n            enriched_base + raw_base +\n            enriched_ici + raw_ici +\n            enriched_numerical + raw_numerical +\n            enriched_percentage + raw_percentage +\n            enriched_year + raw_year +\n            visual_bonus + financial_bonus\n        )\n        \n        scored_chunks.append((total_score, chunk))\n    \n    # Sort by score and take top chunks\n    scored_chunks.sort(key=lambda x: x[0], reverse=True)\n    \n    # BALANCED SELECTION: Ensure mix of high-context and high-precision chunks\n    selected_chunks = []\n    enriched_heavy = 0\n    raw_heavy = 0\n    \n    for score, chunk in scored_chunks[:max_chunks * 2]:  # Consider more candidates\n        if len(selected_chunks) >= max_chunks:\n            break\n            \n        enriched_text = chunk.get(\"ENRICHED_CHUNK\", \"\")\n        raw_text = chunk.get(\"RAW_CHUNK_TEXT\", \"\")\n        \n        # Determine if chunk is enriched-heavy or raw-heavy based on content length/richness\n        is_enriched_heavy = len(enriched_text) > len(raw_text) * 2\n        is_raw_heavy = len(raw_text) > 100 and any(char.isdigit() for char in raw_text)\n        \n        # Balance selection\n        if is_enriched_heavy and enriched_heavy < max_chunks * 0.6:  # Up to 60% enriched\n            selected_chunks.append(chunk)\n            enriched_heavy += 1\n        elif is_raw_heavy and raw_heavy < max_chunks * 0.5:  # Up to 50% raw-focused\n            selected_chunks.append(chunk)\n            raw_heavy += 1\n        elif len(selected_chunks) < max_chunks:  # Fill remaining slots\n            selected_chunks.append(chunk)\n    \n    return selected_chunks ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a3359b5-43db-4b12-994a-c538445dfcdf",
   "metadata": {
    "name": "MD_streamlit_interface",
    "collapsed": false
   },
   "source": "## üìò Streamlit App:  Q\\&A Workflow\n\n\n### üî¢ **Workflow Overview**\n\nThe pipeline consists of **7 sequential steps**, shown with real-time status updates in the UI:\n\n---\n\n### 1. **Document Search (Cortex Semantic Search)**\n\n* The user enters a question via `st.chat_input()`.\n* The question is normalized using `rephrase_for_search()`.\n* `query_search_service()` performs an **embedding + keyword** search using the Cortex Search Service.\n* Returns up to 50 enriched document chunks with metadata, image filenames, and associated PDF references.\n* If no chunks are found, the assistant replies with a fallback message.\n\n---\n\n### 2. **Answer Generation from Text Chunks**\n\n* `ai_complete_on_text()` receives the retrieved chunks and builds a prompt with:\n\n  * **Clickable presigned URLs**\n  * Clean markdown formatting\n  * Strong precision constraints to avoid hallucinations\n* The model (`claude-3-7-sonnet`) returns:\n\n  * A **direct answer**\n  * A **confidence score**\n  * A **justification**\n  * **CITED SOURCES** with Markdown links\n\n---\n\n### 3. **Image Deduplication**\n\n* To reduce cost and noise, `(PDF_FILE_NAME, IMAGE_FILE_NAME)` pairs are deduplicated.\n* This ensures each page is only submitted once for image-based validation.\n\n---\n\n### 4. **Extract Cited Sources & Pages**\n\n* The model's output from Step 2 is parsed using `extract_cited_docs_and_pages()` to identify only the **relevant documents and pages** to check against image data.\n* Uses regex to extract all `Document - page X` citations for focused validation.\n\n---\n\n### 5. **Filter Images to Cited Pages**\n\n* The deduplicated images are **filtered** to those matching the cited documents and pages.\n* Matching is done via fuzzy string matching and page number extraction.\n* Each matched page is then submitted to Cortex with `ai_complete_on_image_async()` for image critique.\n\n---\n\n### 6. **Run Image-Based Validation (Async)**\n\n* Cortex jobs are resolved using `resolve_async_job()` and visual progress is displayed.\n* Errors are handled gracefully, and failed results are tagged with placeholders.\n* The results include `RESULT`, `PRESIGNED_URL`, and other metadata fields.\n\n---\n\n### 7. **Final Answer Synthesis**\n\n* All validated image responses are passed to `synthesise_all_answers()` along with the original text answer.\n* The synthesis prompt:\n\n  * Merges, reconciles, and improves factual accuracy.\n  * Rephrases the result clearly for human readers.\n  * **Cites each document+page with a working hyperlink**.\n* The result is then displayed to the user in the chat.\n\n---"
  },
  {
   "cell_type": "code",
   "id": "eea3ad13-7e91-407b-9d5f-c2eb38822aa4",
   "metadata": {
    "language": "python",
    "name": "PY_streamlit_interface"
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport json\nimport time\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport re\nfrom snowflake.core import Root\n\nst.title(\"üè¶ Financial Document AI Assistant\")\n\n# Initialize session state for custom questions\nif 'custom_results' not in st.session_state:\n    st.session_state.custom_results = {}\n\n# Sidebar settings\nwith st.sidebar:\n    st.markdown(\"## ‚öôÔ∏è Performance Settings\")\n    st.markdown(\"**Image Analysis:**\")\n    MAX_IMAGES_TO_ANALYZE = st.slider(\"Max Images to Analyze\", 1, 20, 8)\n    st.write(f\"Currently analyzing top {MAX_IMAGES_TO_ANALYZE} images\")\n    \n    if st.button(\"üîÑ Reset Settings\"):\n        st.experimental_rerun()\n\n# User question input\nuser_question = st.text_input(\n    \"üí≠ Ask a question about financial documents:\",\n    placeholder=\"e.g., What are the trends in mutual fund assets?\"\n)\n\n# Add submit button\ncol1, col2 = st.columns([3, 1])\nwith col2:\n    submit_clicked = st.button(\"üîç Ask Question\", type=\"primary\")\n\nif user_question and submit_clicked:\n    with st.spinner(\"üîç Processing your question...\"):\n        start_time = time.time()\n        \n        # Step 1: Search\n        st.write(\"üîç Step 1 of 7: Searching vector database...\")\n        # Initialize the search service\n        root = Root(sp_session)\n        search_service = (root\n            .databases[\"CORTEX_SEARCH_TUTORIAL_DB\"]\n            .schemas[\"PUBLIC\"]\n            .cortex_search_services[\"DOCS_SEARCH_SERVICE\"]\n        )\n        search_results = query_multi_index_search_service(sp_session, search_service, user_question)\n        \n        # Debug: Check what we got from search\n        st.write(f\"Debug - Search results type: {type(search_results)}\")\n        \n        # Parse JSON string to Python object if needed\n        if isinstance(search_results, str):\n            search_results = json.loads(search_results)\n        \n        # The search results likely come as a nested structure, extract the actual results\n        if isinstance(search_results, dict) and 'results' in search_results:\n            search_results = search_results['results']\n        elif isinstance(search_results, dict) and 'data' in search_results:\n            search_results = search_results['data']\n        \n        st.write(f\"Debug - After parsing, type: {type(search_results)}, length: {len(search_results) if hasattr(search_results, '__len__') else 'N/A'}\")\n        \n        # Step 2: Smart chunk selection\n        st.write(\"üß† Step 2 of 7: Smart chunk selection...\")\n        deduped_results = smart_chunk_selection(search_results, user_question)\n        \n        # Step 3: Text analysis\n        st.write(\"üìù Step 3 of 7: Analyzing text content...\")\n        # Pass the full chunk objects, not just the enriched text\n        answer_text = ai_complete_on_text(sp_session, user_question, deduped_results)\n        \n        # Step 4: Extract citations\n        st.write(\"üìö Step 4 of 7: Extracting citations...\")\n        # Extract the result string from the dictionary returned by ai_complete_on_text\n        answer_text_str = answer_text.get(\"result\", \"\") if isinstance(answer_text, dict) else str(answer_text)\n        cited_docs_pages = extract_cited_docs_and_pages(answer_text_str)\n        \n        # Step 5: Match images\n        st.write(\"üñºÔ∏è Step 5 of 7: Matching relevant images...\")\n        \n        # Smart image scoring function\n        def score_image_relevance(item, question):\n            \"\"\"Score image relevance based on multiple factors\"\"\"\n            score = 0\n            question_lower = question.lower()\n            content = item.get('ENRICHED_CHUNK', '').lower()\n            raw_content = item.get('RAW_CHUNK_TEXT', '').lower()\n            \n            # Financial keywords boost\n            financial_terms = ['trillion', 'billion', 'million', 'percent', '%', 'assets', 'funds', 'investment', 'expense', 'ratio', 'market', 'share']\n            for term in financial_terms:\n                if term in content or term in raw_content:\n                    score += 10\n            \n            # Question keyword overlap\n            question_words = set(question_lower.split())\n            content_words = set(content.split())\n            overlap = len(question_words.intersection(content_words))\n            score += overlap * 5\n            \n            # Numerical content bonus\n            if any(char.isdigit() for char in content):\n                score += 20\n            \n            # Chart/table indicators\n            chart_indicators = ['chart', 'table', 'figure', 'graph', 'data']\n            for indicator in chart_indicators:\n                if indicator in content:\n                    score += 15\n            \n            return score\n        \n        # Get all available images and score them\n        all_images = [result for result in deduped_results if result.get('IMAGE_FILE_NAME')]\n        \n        if all_images:\n            # Score and sort images\n            scored_images = [(item, score_image_relevance(item, user_question)) for item in all_images]\n            scored_images.sort(key=lambda x: x[1], reverse=True)\n            \n            # Take top N images\n            matched_images = [item for item, score in scored_images[:MAX_IMAGES_TO_ANALYZE]]\n            \n            st.write(f\"Found {len(all_images)} total images, analyzing top {len(matched_images)} most relevant\")\n        else:\n            matched_images = []\n            st.write(\"No images found for analysis\")\n        \n        # Step 6: Process citations and create fallback if needed\n        st.write(\"‚öôÔ∏è Step 6 of 7: Processing citations...\")\n        \n        if not cited_docs_pages:\n            st.write(\"‚ö†Ô∏è No citations found - activating fallback mode\")\n            # Create fallback citations from available images\n            for result in matched_images[:5]:  # Limit fallback to top 5\n                doc_name = result.get('ORIGINAL_FILE_NAME', 'Unknown')\n                page_num = str(result.get('PAGE_NUMBER', 0))\n                if doc_name not in cited_docs_pages:\n                    cited_docs_pages[doc_name] = set()\n                cited_docs_pages[doc_name].add(page_num)\n            \n            st.write(f\"‚úÖ Created fallback citations for {len(cited_docs_pages)} documents\")\n        \n        # Step 7: Synthesize\n        st.write(\"üß™ Step 7 of 7: Synthesize final answer\")\n        st.write(\"Synthesizing text + image answers into a final response...\")\n        \n        # Process images with progress tracking\n        image_critiques = []\n        if matched_images:\n            progress_placeholder = st.empty()\n            \n            def process_limited_images():\n                \"\"\"Process images synchronously using Snowpark jobs\"\"\"\n                critiques = []\n                \n                for i, result in enumerate(matched_images):\n                    progress_placeholder.text(f\"Processing image critiques... ({i+1}/{len(matched_images)})\")\n                    \n                    # Create Snowpark job for this image\n                    job = ai_complete_on_image_async(sp_session, user_question, result, answer_text)\n                    \n                    # Resolve the job synchronously\n                    resolved_result = resolve_async_job(job)\n                    critique = resolved_result.get(\"RESULT\", \"\") if resolved_result else \"\"\n                    \n                    if critique and critique.strip():\n                        critiques.append(critique)\n                \n                return critiques\n            \n            # Run image processing\n            image_critiques = process_limited_images()\n            progress_placeholder.empty()\n        \n        # Combine text and image results\n        final_answer = answer_text_str\n        if image_critiques:\n            combined_critique = '\\n\\n'.join([c for c in image_critiques if c and c.strip()])\n            if combined_critique:\n                final_answer += f\"\\n\\n**Additional Image Analysis:**\\n{combined_critique}\"\n        \n        # Display final answer\n        st.markdown(\"## üéØ Final Answer:\")\n        st.markdown(final_answer)\n        \n        # Performance metrics\n        total_time = time.time() - start_time\n        st.markdown(f\"‚è±Ô∏è **Total time taken:** {total_time:.2f} seconds\")\n    \n    # Debug and additional analysis sections\n    with st.expander(\"üîß Debug - Pipeline Diagnostics\"):\n        st.write(\"**üîç DIAGNOSTIC - Search Results:**\")\n        st.write(f\"Total search results: {len(search_results) if search_results else 0}\")\n        st.write(f\"After smart selection: {len(deduped_results) if deduped_results else 0}\")\n        \n        st.write(\"**üîç DIAGNOSTIC - Extracted Citations:**\")\n        st.write(f\"Citations found: {dict(cited_docs_pages) if cited_docs_pages else {}}\")\n        st.write(f\"Number of cited documents: {len(cited_docs_pages) if cited_docs_pages else 0}\")\n        \n        st.write(\"**üîç DIAGNOSTIC - Text Answer Sample:**\")\n        st.write(f\"Answer text (first 500 chars): {answer_text_str[:500]}...\")\n        \n        st.write(\"**üîç DIAGNOSTIC - Available Images:**\")\n        for i, result in enumerate(matched_images[:5]):\n            doc_name = result.get('ORIGINAL_FILE_NAME', 'Unknown')\n            img_file = result.get('IMAGE_FILE_NAME', 'Unknown')\n            st.write(f\"Image {i+1}: {doc_name} -> {img_file}\")\n        \n        st.write(\"**üîç DIAGNOSTIC - Final Results:**\")\n        st.write(f\"Total jobs created: {len(image_critiques)}\")\n        st.write(f\"Successful critiques: {len([c for c in image_critiques if c and c.strip()])}\")\n    \n    with st.expander(\"üîç Debug - Raw Image Answers\"):\n        if matched_images:\n            st.write(f\"Found {len(matched_images)} relevant images for analysis:\")\n            \n            for i, ans in enumerate(matched_images):\n                st.markdown(f\"### üìÑ **Image {i+1}**\")\n                \n                # Display image metadata\n                col1, col2 = st.columns([1, 1])\n                with col1:\n                    st.write(f\"**üìÑ Document:** {ans.get('ORIGINAL_FILE_NAME', 'Unknown')}\")\n                    st.write(f\"**üñºÔ∏è Image File:** {ans.get('IMAGE_FILE_NAME', 'Unknown')}\")\n                    st.write(f\"**üìÑ Page:** {ans.get('PAGE_NUMBER', 'Unknown')}\")\n                \n                with col2:\n                    # Display content preview\n                    content_preview = ans.get('ENRICHED_CHUNK', 'No content available')[:200]\n                    st.write(f\"**üìù Content Preview:** {content_preview}...\")\n                \n                # Display the actual image\n                try:\n                    image_file_name = ans.get('IMAGE_FILE_NAME')\n                    if image_file_name:\n                        image_path = f\"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/{image_file_name}\"\n                        \n                        try:\n                            # Use get_stream() for Snowflake stage files\n                            with sp_session.file.get_stream(image_path, decompress=False) as stream:\n                                image_bytes = stream.read()\n                            st.image(image_bytes, caption=f\"Page {ans.get('PAGE_NUMBER', 'Unknown')}\", width=300)\n                        except Exception as e:\n                            st.error(f\"Could not load image: {e}\")\n                            st.write(f\"Attempted path: {image_path}\")\n                            st.write(f\"Debug Info:\")\n                            st.write(f\"image_file: {ans.get('IMAGE_FILE_NAME')}\")\n                            st.write(f\"Available ans keys: {list(ans.keys())}\")\n                except Exception as e:\n                    st.error(f\"Error processing image: {e}\")\n                \n                # Custom question section with session state\n                st.markdown(\"---\")\n                st.markdown(\"**ü§î Ask a custom question about this specific image:**\")\n                \n                # Create unique keys for this image\n                question_key = f\"img_question_{i}\"\n                result_key = f\"img_result_{i}\"\n                \n                # Custom question input\n                custom_question = st.text_input(\n                    \"Your question:\",\n                    key=question_key,\n                    placeholder=\"e.g., What are the exact numbers in this chart?\",\n                    help=\"Ask specific questions about this image to get targeted analysis\"\n                )\n                \n                col_btn1, col_btn2 = st.columns([1, 1])\n                \n                with col_btn1:\n                    # Direct analysis without page refresh\n                    if st.button(f\"üîç Analyze Image\", key=f\"analyze_btn_{i}\"):\n                        if custom_question:\n                            with st.spinner(\"üß† Analyzing image with your question...\"):\n                                try:\n                                    # Run AI_COMPLETE on the specific image with custom question\n                                    mock_text_answer = {\"result\": f\"Custom question: {custom_question}\"}\n                                    job = ai_complete_on_image_async(sp_session, custom_question, ans, mock_text_answer)\n                                    resolved_result = resolve_async_job(job)\n                                    custom_critique = resolved_result.get(\"RESULT\", \"\") if resolved_result else \"\"\n                                    \n                                    # Store result in session state\n                                    st.session_state[result_key] = custom_critique\n                                    st.success(\"Analysis complete! Check result below.\")\n                                    \n                                except Exception as e:\n                                    st.error(f\"Error analyzing image: {e}\")\n                        else:\n                            st.warning(\"Please enter a question first.\")\n                \n                # Display stored result if available\n                if result_key in st.session_state and st.session_state[result_key]:\n                    st.success(\"**üéØ Custom AI Analysis:**\")\n                    st.markdown(st.session_state[result_key])\n                \n                with col_btn2:\n                    # Clear result button\n                    if st.button(f\"üóëÔ∏è Clear\", key=f\"clear_btn_{i}\"):\n                        # Clear stored results for this image\n                        if result_key in st.session_state:\n                            del st.session_state[result_key]\n                        st.success(\"Result cleared!\")\n                \n                st.markdown(\"---\")\n        else:\n            st.write(\"No images available for analysis.\")\n    \n    with st.expander(\"üìä Debug - Hybrid Text Analysis\"):\n        if deduped_results:\n            st.write(\"**Enriched vs Raw Content Comparison:**\")\n            for i, chunk in enumerate(deduped_results[:5]):  # Show first 5 chunks\n                st.markdown(f\"### Chunk {i+1}\")\n                \n                col1, col2 = st.columns(2)\n                with col1:\n                    st.markdown(\"**ü§ñ Enriched Content:**\")\n                    enriched = chunk.get('ENRICHED_CHUNK', 'N/A')[:300]\n                    st.text_area(\"\", enriched, height=100, key=f\"enriched_{i}\", disabled=True)\n                \n                with col2:\n                    st.markdown(\"**üìÑ Raw Content:**\") \n                    raw = chunk.get('RAW_CHUNK_TEXT', 'N/A')[:300]\n                    st.text_area(\"\", raw, height=100, key=f\"raw_{i}\", disabled=True)\n                \n                st.markdown(\"---\")\n\nst.markdown(\"---\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "20e56d0d-9b29-4f99-96e5-14edac5e44cc",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "Give me a break down of the company's total net investments by percentage of asset class. \n"
  },
  {
   "cell_type": "code",
   "id": "60c77d92-f1e0-4a65-b374-63168ac94a5f",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "import streamlit as st\nimport pandas as pd\nimport json\nimport time\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport re\nfrom snowflake.core import Root\n\nst.title(\"üè¶ Financial Document AI Assistant\")\n\n# Initialize session state for custom questions\nif 'custom_results' not in st.session_state:\n    st.session_state.custom_results = {}\n\n# Sidebar settings\nwith st.sidebar:\n    st.markdown(\"## ‚öôÔ∏è Performance Settings\")\n    st.markdown(\"**Image Analysis:**\")\n    MAX_IMAGES_TO_ANALYZE = st.slider(\"Max Images to Analyze\", 1, 20, 8)\n    st.write(f\"Currently analyzing top {MAX_IMAGES_TO_ANALYZE} images\")\n    \n    if st.button(\"üîÑ Reset Settings\"):\n        st.experimental_rerun()\n\n# User question input\nuser_question = st.text_input(\n    \"üí≠ Ask a question about financial documents:\",\n    placeholder=\"e.g., What are the trends in mutual fund assets?\"\n)\n\n# Add submit button\ncol1, col2 = st.columns([3, 1])\nwith col2:\n    submit_clicked = st.button(\"üîç Ask Question\", type=\"primary\")\n\nif user_question and submit_clicked:\n    with st.spinner(\"üîç Processing your question...\"):\n        start_time = time.time()\n        \n        # Step 1: Search\n        st.write(\"üîç Step 1 of 7: Searching vector database...\")\n        # Initialize the search service\n        root = Root(sp_session)\n        search_service = (root\n            .databases[\"CORTEX_SEARCH_TUTORIAL_DB\"]\n            .schemas[\"PUBLIC\"]\n            .cortex_search_services[\"DOCS_SEARCH_SERVICE\"]\n        )\n        search_results = query_multi_index_search_service(sp_session, search_service, user_question)\n        \n        # Debug: Check what we got from search\n        st.write(f\"Debug - Search results type: {type(search_results)}\")\n        \n        # Parse JSON string to Python object if needed\n        if isinstance(search_results, str):\n            search_results = json.loads(search_results)\n        \n        # The search results likely come as a nested structure, extract the actual results\n        if isinstance(search_results, dict) and 'results' in search_results:\n            search_results = search_results['results']\n        elif isinstance(search_results, dict) and 'data' in search_results:\n            search_results = search_results['data']\n        \n        st.write(f\"Debug - After parsing, type: {type(search_results)}, length: {len(search_results) if hasattr(search_results, '__len__') else 'N/A'}\")\n        \n        # Step 2: Smart chunk selection\n        st.write(\"üß† Step 2 of 7: Smart chunk selection...\")\n        deduped_results = smart_chunk_selection(search_results, user_question)\n        \n        # Step 3: Text analysis\n        st.write(\"üìù Step 3 of 7: Analyzing text content...\")\n        # Pass the full chunk objects, not just the enriched text\n        answer_text = ai_complete_on_text(sp_session, user_question, deduped_results)\n        \n        # Step 4: Extract citations\n        st.write(\"üìö Step 4 of 7: Extracting citations...\")\n        # Extract the result string from the dictionary returned by ai_complete_on_text\n        answer_text_str = answer_text.get(\"result\", \"\") if isinstance(answer_text, dict) else str(answer_text)\n        cited_docs_pages = extract_cited_docs_and_pages(answer_text_str)\n        \n        # Step 5: Match images\n        st.write(\"üñºÔ∏è Step 5 of 7: Matching relevant images...\")\n        \n        # Smart image scoring function\n        def score_image_relevance(item, question):\n            \"\"\"Score image relevance based on multiple factors\"\"\"\n            score = 0\n            question_lower = question.lower()\n            content = item.get('ENRICHED_CHUNK', '').lower()\n            raw_content = item.get('RAW_CHUNK_TEXT', '').lower()\n            \n            # Financial keywords boost\n            financial_terms = ['trillion', 'billion', 'million', 'percent', '%', 'assets', 'funds', 'investment', 'expense', 'ratio', 'market', 'share']\n            for term in financial_terms:\n                if term in content or term in raw_content:\n                    score += 10\n            \n            # Question keyword overlap\n            question_words = set(question_lower.split())\n            content_words = set(content.split())\n            overlap = len(question_words.intersection(content_words))\n            score += overlap * 5\n            \n            # Numerical content bonus\n            if any(char.isdigit() for char in content):\n                score += 20\n            \n            # Chart/table indicators\n            chart_indicators = ['chart', 'table', 'figure', 'graph', 'data']\n            for indicator in chart_indicators:\n                if indicator in content:\n                    score += 15\n            \n            return score\n        \n        # Get all available images and score them\n        all_images = [result for result in deduped_results if result.get('IMAGE_FILE_NAME')]\n        \n        if all_images:\n            # Score and sort images\n            scored_images = [(item, score_image_relevance(item, user_question)) for item in all_images]\n            scored_images.sort(key=lambda x: x[1], reverse=True)\n            \n            # Take top N images\n            matched_images = [item for item, score in scored_images[:MAX_IMAGES_TO_ANALYZE]]\n            \n            st.write(f\"Found {len(all_images)} total images, analyzing top {len(matched_images)} most relevant\")\n        else:\n            matched_images = []\n            st.write(\"No images found for analysis\")\n        \n        # Step 6: Process citations and create fallback if needed\n        st.write(\"‚öôÔ∏è Step 6 of 7: Processing citations...\")\n        \n        if not cited_docs_pages:\n            st.write(\"‚ö†Ô∏è No citations found - activating fallback mode\")\n            # Create fallback citations from available images\n            for result in matched_images[:5]:  # Limit fallback to top 5\n                doc_name = result.get('ORIGINAL_FILE_NAME', 'Unknown')\n                page_num = str(result.get('PAGE_NUMBER', 0))\n                if doc_name not in cited_docs_pages:\n                    cited_docs_pages[doc_name] = set()\n                cited_docs_pages[doc_name].add(page_num)\n            \n            st.write(f\"‚úÖ Created fallback citations for {len(cited_docs_pages)} documents\")\n        \n        # Step 7: Synthesize\n        st.write(\"üß™ Step 7 of 7: Synthesize final answer\")\n        st.write(\"Synthesizing text + image answers into a final response...\")\n        \n        # Process images with progress tracking\n        image_critiques = []\n        if matched_images:\n            progress_placeholder = st.empty()\n            \n            def process_limited_images():\n                \"\"\"Process images synchronously using Snowpark jobs\"\"\"\n                critiques = []\n                \n                for i, result in enumerate(matched_images):\n                    progress_placeholder.text(f\"Processing image critiques... ({i+1}/{len(matched_images)})\")\n                    \n                    # Create Snowpark job for this image\n                    job = ai_complete_on_image_async(sp_session, user_question, result, answer_text)\n                    \n                    # Resolve the job synchronously\n                    resolved_result = resolve_async_job(job)\n                    critique = resolved_result.get(\"RESULT\", \"\") if resolved_result else \"\"\n                    \n                    if critique and critique.strip():\n                        critiques.append(critique)\n                \n                return critiques\n            \n            # Run image processing\n            image_critiques = process_limited_images()\n            progress_placeholder.empty()\n        \n        # Combine text and image results\n        final_answer = answer_text_str\n        if image_critiques:\n            combined_critique = '\\n\\n'.join([c for c in image_critiques if c and c.strip()])\n            if combined_critique:\n                final_answer += f\"\\n\\n**Additional Image Analysis:**\\n{combined_critique}\"\n        \n        # Display final answer\n        st.markdown(\"## üéØ Final Answer:\")\n        st.markdown(final_answer)\n        \n        # Performance metrics\n        total_time = time.time() - start_time\n        st.markdown(f\"‚è±Ô∏è **Total time taken:** {total_time:.2f} seconds\")\n    \n    # Debug and additional analysis sections\n    with st.expander(\"üîß Debug - Pipeline Diagnostics\"):\n        st.write(\"**üîç DIAGNOSTIC - Search Results:**\")\n        st.write(f\"Total search results: {len(search_results) if search_results else 0}\")\n        st.write(f\"After smart selection: {len(deduped_results) if deduped_results else 0}\")\n        \n        st.write(\"**üîç DIAGNOSTIC - Extracted Citations:**\")\n        st.write(f\"Citations found: {dict(cited_docs_pages) if cited_docs_pages else {}}\")\n        st.write(f\"Number of cited documents: {len(cited_docs_pages) if cited_docs_pages else 0}\")\n        \n        st.write(\"**üîç DIAGNOSTIC - Text Answer Sample:**\")\n        st.write(f\"Answer text (first 500 chars): {answer_text_str[:500]}...\")\n        \n        st.write(\"**üîç DIAGNOSTIC - Available Images:**\")\n        for i, result in enumerate(matched_images[:5]):\n            doc_name = result.get('ORIGINAL_FILE_NAME', 'Unknown')\n            img_file = result.get('IMAGE_FILE_NAME', 'Unknown')\n            st.write(f\"Image {i+1}: {doc_name} -> {img_file}\")\n        \n        st.write(\"**üîç DIAGNOSTIC - Final Results:**\")\n        st.write(f\"Total jobs created: {len(image_critiques)}\")\n        st.write(f\"Successful critiques: {len([c for c in image_critiques if c and c.strip()])}\")\n    \n    with st.expander(\"üîç Debug - Raw Image Answers\"):\n        if matched_images:\n            st.write(f\"Found {len(matched_images)} relevant images for analysis:\")\n            \n            for i, ans in enumerate(matched_images):\n                st.markdown(f\"### üìÑ **Image {i+1}**\")\n                \n                # Display image metadata\n                col1, col2 = st.columns([1, 1])\n                with col1:\n                    st.write(f\"**üìÑ Document:** {ans.get('ORIGINAL_FILE_NAME', 'Unknown')}\")\n                    st.write(f\"**üñºÔ∏è Image File:** {ans.get('IMAGE_FILE_NAME', 'Unknown')}\")\n                    st.write(f\"**üìÑ Page:** {ans.get('PAGE_NUMBER', 'Unknown')}\")\n                \n                with col2:\n                    # Display content preview\n                    content_preview = ans.get('ENRICHED_CHUNK', 'No content available')[:200]\n                    st.write(f\"**üìù Content Preview:** {content_preview}...\")\n                \n                # Display the actual image\n                try:\n                    image_file_name = ans.get('IMAGE_FILE_NAME')\n                    if image_file_name:\n                        image_path = f\"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/{image_file_name}\"\n                        \n                        try:\n                            # Use get_stream() for Snowflake stage files\n                            with sp_session.file.get_stream(image_path, decompress=False) as stream:\n                                image_bytes = stream.read()\n                            st.image(image_bytes, caption=f\"Page {ans.get('PAGE_NUMBER', 'Unknown')}\", width=300)\n                        except Exception as e:\n                            st.error(f\"Could not load image: {e}\")\n                            st.write(f\"Attempted path: {image_path}\")\n                            st.write(f\"Debug Info:\")\n                            st.write(f\"image_file: {ans.get('IMAGE_FILE_NAME')}\")\n                            st.write(f\"Available ans keys: {list(ans.keys())}\")\n                except Exception as e:\n                    st.error(f\"Error processing image: {e}\")\n                \n                # Custom question section with session state\n                st.markdown(\"---\")\n                st.markdown(\"**ü§î Ask a custom question about this specific image:**\")\n                \n                # Create unique keys for this image\n                question_key = f\"img_question_{i}\"\n                result_key = f\"img_result_{i}\"\n                \n                # Custom question input\n                custom_question = st.text_input(\n                    \"Your question:\",\n                    key=question_key,\n                    placeholder=\"e.g., What are the exact numbers in this chart?\",\n                    help=\"Ask specific questions about this image to get targeted analysis\"\n                )\n                \n                col_btn1, col_btn2 = st.columns([1, 1])\n                \n                with col_btn1:\n                    # Direct analysis without page refresh\n                    if st.button(f\"üîç Analyze Image\", key=f\"analyze_btn_{i}\"):\n                        if custom_question:\n                            with st.spinner(\"üß† Analyzing image with your question...\"):\n                                try:\n                                    # Run AI_COMPLETE on the specific image with custom question\n                                    mock_text_answer = {\"result\": f\"Custom question: {custom_question}\"}\n                                    job = ai_complete_on_image_async(sp_session, custom_question, ans, mock_text_answer)\n                                    resolved_result = resolve_async_job(job)\n                                    custom_critique = resolved_result.get(\"RESULT\", \"\") if resolved_result else \"\"\n                                    \n                                    # Store result in session state\n                                    st.session_state[result_key] = custom_critique\n                                    st.success(\"Analysis complete! Check result below.\")\n                                    \n                                except Exception as e:\n                                    st.error(f\"Error analyzing image: {e}\")\n                        else:\n                            st.warning(\"Please enter a question first.\")\n                \n                # Display stored result if available\n                if result_key in st.session_state and st.session_state[result_key]:\n                    st.success(\"**üéØ Custom AI Analysis:**\")\n                    st.markdown(st.session_state[result_key])\n                \n                with col_btn2:\n                    # Clear result button\n                    if st.button(f\"üóëÔ∏è Clear\", key=f\"clear_btn_{i}\"):\n                        # Clear stored results for this image\n                        if result_key in st.session_state:\n                            del st.session_state[result_key]\n                        st.success(\"Result cleared!\")\n                \n                st.markdown(\"---\")\n        else:\n            st.write(\"No images available for analysis.\")\n    \n    with st.expander(\"üìä Debug - Hybrid Text Analysis\"):\n        if deduped_results:\n            st.write(\"**Enriched vs Raw Content Comparison:**\")\n            for i, chunk in enumerate(deduped_results[:5]):  # Show first 5 chunks\n                st.markdown(f\"### Chunk {i+1}\")\n                \n                col1, col2 = st.columns(2)\n                with col1:\n                    st.markdown(\"**ü§ñ Enriched Content:**\")\n                    enriched = chunk.get('ENRICHED_CHUNK', 'N/A')[:300]\n                    st.text_area(\"\", enriched, height=100, key=f\"enriched_{i}\", disabled=True)\n                \n                with col2:\n                    st.markdown(\"**üìÑ Raw Content:**\") \n                    raw = chunk.get('RAW_CHUNK_TEXT', 'N/A')[:300]\n                    st.text_area(\"\", raw, height=100, key=f\"raw_{i}\", disabled=True)\n                \n                st.markdown(\"---\")\n\nst.markdown(\"---\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e45d952-6231-4f12-aa4c-ff1b371235ac",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "what is the expense ratio trends in equity for actively managed mutual funds?"
  }
 ]
}