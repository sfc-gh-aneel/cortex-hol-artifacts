{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "e7pqucbr6y23rwsjadb2",
   "authorId": "507732859711",
   "authorName": "ADMIN",
   "authorEmail": "adam.neel@snowflake.com",
   "sessionId": "1dab902c-19c4-4928-a859-9809a2c23968",
   "lastEditTime": 1754581972213
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9923d8-945b-4b4b-8d37-3a48807cafa3",
   "metadata": {
    "name": "MD_intro",
    "collapsed": false
   },
   "source": "# 🧠 Seclock Document Q&A – Multimodal Document Pipeline (Streamlit UI)\n\nThis script powers an interactive Streamlit app that answers technical questions about Seclock’s door hardware documents using **multimodal AI**—combining vector search, LLM reasoning, and image validation via Snowflake Cortex.\n\n## 🔧 High-Level Features\n\n- 📥 **Document Ingestion & Processing**\n  - Splits PDFs into per-page images and text using `parse_document()`.\n  - Stores images in a Snowflake stage and text/metadata in structured tables.\n  - Generates image embeddings using `embed_image_1024`.\n---\n- 🔍 **Semantic Search & Retrieval**\n  - Converts user questions into temporary query images to leverage image embeddings.\n  - Embeds the image and performs vector search via Cortex Search Service.\n  - Retrieves top matching enriched chunks (text + metadata + image reference).\n---\n- 🧠 **LLM-Based Text Answering**\n  - Feeds retrieved context into Claude (`claude-3-7-sonnet`) to generate:\n    - Direct answers\n    - Confidence scores\n    - Justifications\n    - Markdown-linked citations (`[Document - page X](presigned_url)`)\n---\n- 🖼️ **Image-Based Reasoning**\n  - Identifies cited pages from the text answer and filters matching images.\n  - Submits images asynchronously to Cortex for answer validation.\n  - Uses image metadata and visual context to extract direct answers or critiques.\n---\n- 🧪 **Answer Synthesis**\n  - Combines text and image-based answers into a final, human-readable response.\n  - Rewrites for clarity, accuracy, and directness.\n  - Prioritizes newer or more reliable sources and flags conflicting evidence.\n  - Provides full citation trail with links to specific document pages.\n---\n- 🔗 **Embedding & Retrieval Enrichment**\n  - Merges document metadata, summaries, page-level chunks, and vectors into a retrieval-optimized format.\n  - Allows precise filtering and future expansion (e.g., by product line or brand).\n---\n- 💬 **Streamlit Chat UI**\n  - Provides an interactive chatbot interface with step-by-step progress feedback.\n  - Includes expandable debug sections for:\n    - Source documents used\n    - Text answer reasoning\n    - Image-based critiques\n    - LLM prompt previews\n"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "PY_imports"
   },
   "source": "# Import python packages\nimport os\nimport sys\nimport json\nimport shutil\nimport datetime\nimport re\nimport time\nimport hashlib\nfrom difflib import SequenceMatcher\nimport tempfile\nfrom textwrap import dedent\nimport streamlit as st\nfrom PIL import Image, ImageDraw, ImageFont\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import List\nfrom typing import Tuple\nimport snowflake.snowpark.session as session\nimport pdfplumber\nimport PyPDF2\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.core import Root\nfrom snowflake.cortex import complete, CompleteOptions\nsp_session = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1cc14c85-301e-4f5a-b7f7-059355aa2230",
   "metadata": {
    "name": "MD_pdf_to_image",
    "collapsed": false
   },
   "source": "## 📄 PDF Preprocessing Pipeline for Document Analysis\n\nPreprocesses PDFs stored in a Snowflake stage, preparing them for downstream AI document analysis. It lists all PDF files in a specified input stage, downloads each file temporarily, and performs two key operations: \n\n1. Splitting the PDF into individual pages and uploading each as a separate PDF file\n2. Converting each page into a high-resolution image, optionally scaled to a maximum dimension, and uploading the images back to a specified output stage. "
  },
  {
   "cell_type": "code",
   "id": "0ad96295-d4e4-43d7-8ab5-62f685fef26f",
   "metadata": {
    "language": "python",
    "name": "PY_pdf_to_image"
   },
   "outputs": [],
   "source": "def print_info(msg: str) -> None:\n    \"\"\"Print info message\"\"\"\n    print(f\"INFO: {msg}\", file=sys.stderr)\n\n\ndef print_error(msg: str) -> None:\n    \"\"\"Print error message\"\"\"\n    print(f\"ERROR: {msg}\", file=sys.stderr)\n    if hasattr(st, \"error\"):\n        st.error(msg)\n\n\ndef print_warning(msg: str) -> None:\n    \"\"\"Print warning message\"\"\"\n    print(f\"WARNING: {msg}\", file=sys.stderr)\n\n\n@dataclass\nclass Config:\n    input_stage: str = \"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/\"\n    output_stage: str = (\n        \"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/PARSED/\"  # Base output stage without subdirectories\n    )\n    input_path: str = \"pre_processed\"\n    output_pdf_path: str = \"paged_pdf\"\n    output_image_path: str = \"paged_image\"\n    allowed_extensions: List[str] = None\n    max_dimension: int = 1500  # Maximum dimension in pixels before scaling\n    dpi: int = 300  # Default DPI for image conversion\n\n    def __post_init__(self):\n        if self.allowed_extensions is None:\n            self.allowed_extensions = [\".pdf\"]\n\n\nclass PDFProcessingError(Exception):\n    \"\"\"Base exception for PDF processing errors\"\"\"\n\n\nclass FileDownloadError(PDFProcessingError):\n    \"\"\"Raised when file download fails\"\"\"\n\n\nclass PDFConversionError(PDFProcessingError):\n    \"\"\"Raised when PDF conversion fails\"\"\"\n\n\n@contextmanager\ndef managed_temp_file(suffix: str = None) -> str:\n    \"\"\"Context manager for temporary file handling\"\"\"\n    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)\n    try:\n        yield temp_file.name\n    finally:\n        # Don't delete the file immediately, let the caller handle cleanup\n        pass\n\n\ndef cleanup_temp_file(file_path: str) -> None:\n    \"\"\"Clean up a temporary file\"\"\"\n    try:\n        if os.path.exists(file_path):\n            os.unlink(file_path)\n    except OSError as e:\n        print_warning(f\"Failed to delete temporary file {file_path}: {e}\")\n\n\ndef list_pdf_files(session: session.Session, config: Config) -> List[dict]:\n    \"\"\"List all PDF files in the source stage\"\"\"\n    try:\n        # Use LIST command instead of DIRECTORY function\n        query = f\"\"\"\n        LIST {config.input_stage}\n        \"\"\"\n\n        file_list = session.sql(query).collect()\n\n        # Filter for PDF files\n        pdf_files = []\n        for file_info in file_list:\n            full_path = file_info[\"name\"]\n            # Extract just the filename from the full path\n            file_name = os.path.basename(full_path)\n\n            if any(\n                file_name.lower().endswith(ext) for ext in config.allowed_extensions\n            ):\n                pdf_files.append(\n                    {\n                        \"RELATIVE_PATH\": file_name,  # Use just the filename\n                        \"FULL_STAGE_PATH\": full_path,  # Use full path for download\n                        \"SIZE\": file_info[\"size\"] if \"size\" in file_info else 0,\n                    }\n                )\n\n        print_info(f\"Found {len(pdf_files)} PDF files in the stage\")\n        return pdf_files\n    except Exception as e:\n        print_error(f\"Failed to list files: {e}\")\n        raise\n\n\ndef download_file_from_stage(\n    session: session.Session, file_path: str, config: Config\n) -> str:\n    \"\"\"Download a file from stage using session.file.get\"\"\"\n    # Create a temporary directory\n    temp_dir = tempfile.mkdtemp()\n    try:\n        # Ensure there are no double slashes in the path\n        stage_path = f\"{config.input_stage.rstrip('/')}/{file_path.lstrip('/')}\"\n\n        # Get the file from stage\n        get_result = session.file.get(stage_path, temp_dir)\n        if not get_result or get_result[0].status != \"DOWNLOADED\":\n            raise FileDownloadError(f\"Failed to download file: {file_path}\")\n\n        # Construct the local path where the file was downloaded\n        local_path = os.path.join(temp_dir, os.path.basename(file_path))\n        if not os.path.exists(local_path):\n            raise FileDownloadError(f\"Downloaded file not found at: {local_path}\")\n\n        return local_path\n    except Exception as e:\n        print_error(f\"Error downloading {file_path}: {e}\")\n        # Clean up the temporary directory\n        try:\n            import shutil\n\n            shutil.rmtree(temp_dir)\n        except Exception as cleanup_error:\n            print_warning(f\"Failed to clean up temporary directory: {cleanup_error}\")\n        raise FileDownloadError(f\"Failed to download file: {e}\")\n\n\ndef upload_file_to_stage(\n    session: session.Session, file_path: str, output_path: str, config: Config\n) -> str:\n    \"\"\"Upload file to the output stage\"\"\"\n    try:\n        # Get the directory and filename from the output path\n        output_dir = os.path.dirname(output_path)\n        base_name = os.path.basename(output_path)\n\n        # Create the full stage path with subdirectory\n        stage_path = f\"{config.output_stage.rstrip('/')}/{output_dir.lstrip('/')}\"\n\n        # Read the content of the original file\n        with open(file_path, \"rb\") as f:\n            file_content = f.read()\n\n        # Create a new file with the correct name\n        temp_dir = tempfile.gettempdir()\n        temp_file_path = os.path.join(temp_dir, base_name)\n\n        # Write the content to the new file\n        with open(temp_file_path, \"wb\") as f:\n            f.write(file_content)\n\n        # Upload the file using session.file.put with compression disabled\n        put_result = session.file.put(\n            temp_file_path, stage_path, auto_compress=False, overwrite=True\n        )\n\n        # Check upload status\n        if not put_result or len(put_result) == 0:\n            raise Exception(f\"Failed to upload file: {base_name}\")\n\n        if put_result[0].status not in [\"UPLOADED\", \"SKIPPED\"]:\n            raise Exception(f\"Upload failed with status: {put_result[0].status}\")\n\n        # Clean up the temporary file\n        if os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n\n        return f\"Successfully uploaded {base_name} to {stage_path}\"\n    except Exception as e:\n        print_error(f\"Error uploading file: {e}\")\n        raise\n\n\ndef process_pdf_files(config: Config) -> None:\n    \"\"\"Main process to orchestrate the PDF splitting\"\"\"\n    try:\n        session = get_active_session()\n        pdf_files = list_pdf_files(session, config)\n\n        for file_info in pdf_files:\n            file_path = file_info[\"RELATIVE_PATH\"]\n            print_info(f\"Processing: {file_path}\")\n\n            try:\n                # Download the PDF file\n                local_pdf_path = download_file_from_stage(session, file_path, config)\n\n                # Get base filename without extension\n                base_name = os.path.splitext(os.path.basename(file_path))[0]\n\n                # Extract individual PDF pages\n                with open(local_pdf_path, \"rb\") as file:\n                    pdf_reader = PyPDF2.PdfReader(file)\n                    num_pages = len(pdf_reader.pages)\n                    print_info(f\"Converting PDF to {num_pages} pages of PDFs\")\n\n                    for i in range(num_pages):\n                        page_num = i + 1\n                        s3_pdf_output_path = (\n                            f\"{config.output_pdf_path}/{base_name}_page_{page_num}.pdf\"\n                        )\n                        pdf_writer = PyPDF2.PdfWriter()\n                        pdf_writer.add_page(pdf_reader.pages[i])\n                        temp_file = tempfile.NamedTemporaryFile(\n                            delete=False, suffix=\".pdf\"\n                        )\n                        local_pdf_tmp_file_name = temp_file.name\n                        with open(local_pdf_tmp_file_name, \"wb\") as output_file:\n                            pdf_writer.write(output_file)\n                        \n                        upload_file_to_stage(\n                            session, local_pdf_tmp_file_name, s3_pdf_output_path, config\n                        )\n                        cleanup_temp_file(local_pdf_tmp_file_name)\n                            \n                # Convert PDF to images                \n                with pdfplumber.open(local_pdf_path) as pdf:\n                    print_info(f\"Converting PDF to {len(pdf.pages)} images\")\n                    for i, page in enumerate(pdf.pages):\n                        page_num = i + 1\n                        # Get page dimensions\n                        width = page.width\n                        height = page.height\n\n                        # Determine if scaling is needed\n                        max_dim = max(width, height)\n                        if max_dim > config.max_dimension:\n                            # Calculate scale factor to fit within max_dimension\n                            scale_factor = config.max_dimension / max_dim\n                            width = int(width * scale_factor)\n                            height = int(height * scale_factor)\n\n                        img = page.to_image(resolution=config.dpi)\n                        temp_file = tempfile.NamedTemporaryFile(\n                            delete=False, suffix=\".png\"\n                        )\n                        local_image_tmp_file_name = temp_file.name\n                        img.save(local_image_tmp_file_name)\n\n                        s3_image_output_path = (\n                            f\"{config.output_image_path}/{base_name}_page_{page_num}.png\"\n                        )\n                        \n                        upload_file_to_stage(\n                            session, local_image_tmp_file_name, s3_image_output_path, config\n                        )\n                        cleanup_temp_file(local_image_tmp_file_name)\n                        \n                # Clean up the original downloaded file\n                cleanup_temp_file(local_pdf_path)\n\n            except Exception as e:\n                print_error(f\"Error processing {file_path}: {e}\")\n                continue\n\n    except Exception as e:\n        print_error(f\"Fatal error in process_pdf_files: {e}\")\n        raise",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7710c338-9693-4d20-8459-3eb567595a0a",
   "metadata": {
    "language": "python",
    "name": "PY_run_pdf_to_image"
   },
   "outputs": [],
   "source": "config = Config(dpi=300)\nprocess_pdf_files(config)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96ff282e-8bbc-4910-be87-f74345c98a7e",
   "metadata": {
    "name": "MD_image_preview",
    "collapsed": false
   },
   "source": "## 🔍 Document Image Preview\n\nTo check everything has been processed as planned, we can look at an image representing a page from the PDFs."
  },
  {
   "cell_type": "code",
   "id": "4eb451f8-d784-47fe-be85-0697507bc2b0",
   "metadata": {
    "language": "python",
    "name": "PY_image_preview"
   },
   "outputs": [],
   "source": "image_path = f\"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/PARSED/paged_image/2023-factbook_page_27.png\"\ntry:\n    image_st = sp_session.file.get_stream( image_path,decompress=False)\nexcept Exception as e:\n    print(\"failed to initialize file stream:\", e)\n    \nimage = image_st.read()\nst.image(image)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d7bbcfc-b7b4-4550-b181-ab9df1e37891",
   "metadata": {
    "language": "sql",
    "name": "SQL_use_role"
   },
   "outputs": [],
   "source": "use role accountadmin",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd639952-f63d-4534-a7ca-13b5c898dbdd",
   "metadata": {
    "name": "MD_image_embedding",
    "collapsed": false
   },
   "source": "## 🧠 Batch Image Embedding with Cortex and Snowpark\n\nThis workflow performs batch image embedding using a Python stored procedure\n\n1. **Identify Unprocessed Images**\n   A temporary table (`limit_directory_table`) is created by listing all image files in the stage (`@utils.ai.stock_ikb_documents/paged_image/`) and filtering out those already embedded in the `output_vector_table`.\n\n2. **Assign Row Numbers for Batching**\n   Each unprocessed image file is assigned a `row_number()` so batches can be defined by row ranges (`start_rn` to `end_rn`).\n\n3. **Define Embedding Procedure**\n   A Python stored procedure `run_image_embedding_batch(start_rn, end_rn)` is created. It:\n\n   * Reads a batch of image files from the temporary table.\n   * Extracts file and metadata (e.g. file name, page number).\n   * Computes an image embedding using `snowflake.cortex.embed_image_1024` with the `voyage-multimodal-3` model.\n   * Saves the embeddings to `output_vector_table`.\n\n4. **Queue Up Batch Jobs**\n   The total number of batches is calculated, and a list of SQL `CALL` statements is built, one per batch.\n\n5. **Run Jobs Concurrently**\n   A loop manages job execution with up to 5 concurrent asynchronous jobs at a time. Each job is submitted using `.collect_nowait()` and polled until it completes.\n\n6. **Monitor and Retry**\n   Each batch is logged upon completion or failure, and the loop continues until all batches are processed.\n\nThis setup allows high-throughput embedding of images inside Snowflake, using Cortex's multimodal capabilities with minimal manual orchestration.\n"
  },
  {
   "cell_type": "code",
   "id": "9a214a51-92d6-42b5-854f-a7da48b04402",
   "metadata": {
    "language": "sql",
    "name": "SQL_image_embedding_sproc"
   },
   "outputs": [],
   "source": "create or replace procedure run_image_embedding_batch(start_rn int, end_rn int)\nreturns string\nlanguage python\nruntime_version = 3.9\npackages = ('snowflake-snowpark-python')\nhandler = 'embed_handler'\nAS\n$$\ndef embed_handler(session, start_rn, end_rn):\n    df = session.sql(f'''\n        select\n            concat('paged_image/', split_part(relative_path, '/', -1)) as file_name,\n            regexp_substr(file_name, 'paged_image/(.*)\\\\.png$', 1, 1, 'e', 1) as paged_file_name,\n            split_part(paged_file_name, '_page_', 0) as original_file_name,\n            split_part(paged_file_name, '_page_', 2)::int as page_number,\n            '@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/' as stage_prefix,\n            to_file(file_url)  as image_file,\n            AI_EMBED(\n                'voyage-multimodal-3', \n                '@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/'||concat('paged_image/', split_part(relative_path, '/', -1))\n            ) as image_vector\n        from limit_directory_table\n        where rn between {start_rn} and {end_rn}\n    ''')\n    ## print(df.columns)\n    df.write.save_as_table(\"OUTPUT_VECTOR_TABLE\", mode=\"append\") ##change this to append\n    return f\"Embedded and saved RN {start_rn} to {end_rn}\"\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0cea428-fea0-428e-b738-a52ecbe8c43c",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "--call run_image_embedding_batch(start_rn int, end_rn int)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a1b1150-0887-4877-9a1d-5f525b54f7fc",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "select get_ddl('table','output_vector_table');\ncreate or replace TABLE OUTPUT_VECTOR_TABLE (\n\tFILE_NAME VARCHAR(16777216),\n\tPAGED_FILE_NAME VARCHAR(16777216),\n\tORIGINAL_FILE_NAME VARCHAR(16777216),\n\tPAGE_NUMBER NUMBER(38,0),\n\tSTAGE_PREFIX VARCHAR(43) NOT NULL,\n\tIMAGE_FILE FILE,\n\tIMAGE_VECTOR VECTOR(FLOAT, 1024)\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5009481b-9c5d-4ea2-81fc-2a8380b8190a",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "select * from output_vector_table",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f45bd12-76ab-4051-b716-6bb080b97ec5",
   "metadata": {
    "language": "python",
    "name": "PY_image_embedding"
   },
   "outputs": [],
   "source": "BATCH_SIZE = 10\nMAX_CONCURRENT = 5\n\n# 1. Create LIMIT_DIRECTORY_TABLE if not exists\nsp_session.sql(\"\"\"\n    create or replace temporary table limit_directory_table as\n    select\n        *,\n        row_number() over (order by relative_path) as rn\n    from\n        directory(@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO)\n    where\n        relative_path like '%paged_image/%'\n        \n\"\"\").collect()\n\n    # create or replace temporary table limit_directory_table as\n    # select\n    #     *,\n    #     row_number() over (order by relative_path) as rn\n    # from\n    #     directory(@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO)\n    # where\n    #     relative_path like '%paged_image/%'\n    #     and\n    #     relative_path not in (\n    #         select file_name from output_vector_table\n\n### The above sql command should be used if we want to only load deltas\n    #     \n\n# 2. Get total batches\nmax_rn = sp_session.sql(\"select max(rn) AS max_rn from limit_directory_table\").collect()[0][\"MAX_RN\"]\ntotal_batches = (max_rn + BATCH_SIZE - 1) // BATCH_SIZE\n\n# 3. Prepare all batch configs\nbatch_queue = []\nfor i in range(total_batches):\n    start_rn = i * BATCH_SIZE + 1\n    end_rn = min((i + 1) * BATCH_SIZE, max_rn)\n    label = f\"Batch {i+1}: RN {start_rn}-{end_rn}\"\n    sql = f\"call run_image_embedding_batch({start_rn}, {end_rn})\"\n    batch_queue.append((sql, label))\n\n# 4. Loop with max 5 concurrent jobs\nactive_jobs = []\n\nwhile batch_queue or active_jobs:\n    # Launch jobs if we have capacity\n    while batch_queue and len(active_jobs) < MAX_CONCURRENT:\n        sql, label = batch_queue.pop(0)\n        print(f\"🚀 Submitting async job for {label}\")\n        try:\n            job = sp_session.sql(sql).collect_nowait()\n            active_jobs.append((job, label))\n        except Exception as e:\n            print(f\"❌ Failed to submit {label}: {e}\")\n\n    # Poll active jobs\n    for job, label in active_jobs.copy():\n        if job.is_done():\n            try:\n                result = job.result()\n                print(f\"✅ {label} completed: {result}\")\n            except Exception as e:\n                print(f\"❌ {label} failed: {e}\")\n            active_jobs.remove((job, label))\n\n    if active_jobs:\n        time.sleep(15)\n\nprint(\"🎉 All batches processed.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c1d767aa-1f24-422e-aef3-9c6ea3e13842",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "select * from output_vector_table",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8dd82d8-665b-4715-8d4d-791776f593a2",
   "metadata": {
    "name": "MD_ocr",
    "collapsed": false
   },
   "source": "## 🔖 Extract Text from PDF Pages\n\nThis SQL script creates a table (`pdf_pages`) that extracts and stores parsed text content from individual PDF pages:\n\n1. **Filter Input Files**\n   It queries the stage `@utils.ai.stock_ikb_documents` and filters files whose path matches the pattern `%paged_pdf/%`, meaning individual page PDFs from previously split documents.\n\n2. **Extract File Metadata**\n   For each PDF file:\n\n   * `file_name` is constructed by prefixing the relative path with `paged_pdf/`.\n   * `paged_file_name` extracts just the PDF filename using regex.\n   * `original_file_name` removes the `_page_X` suffix to get the base document name.\n   * `page_number` is parsed from the filename to track the page.\n\n3. **Generate File References**\n   The `to_file(file_url)` function creates a file object for use in Cortex functions.\n\n4. **Parse PDF Content with Cortex**\n   The `snowflake.cortex.parse_document` function is called on each page to extract its text layout. The result is cast to a string, then parsed as JSON and stored in the `pdf_text` column.\n\n5. **Output the Resulting Table**\n   The final table `pdf_pages` includes:\n\n   * File path and name metadata\n   * Page number\n   * File reference\n   * Structured PDF content parsed by Cortex\n\nThis process enables structured, searchable access to individual page-level text from large documents using Cortex's layout-aware parsing engine.\n"
  },
  {
   "cell_type": "code",
   "id": "c5cc83a2-d562-4a25-b194-05a10e451563",
   "metadata": {
    "language": "sql",
    "name": "SQL_ocr"
   },
   "outputs": [],
   "source": "create or replace table pdf_pages as\nselect\n    concat('PARSED/paged_pdf/', split_part(relative_path, '/', -1)) as file_name,\n    regexp_substr(file_name, 'PARSED/paged_pdf/(.*)\\\\.pdf$', 1, 1, 'e', 1) as paged_file_name,\n    split_part(paged_file_name, '_page_', 0) as original_file_name,\n    split_part(paged_file_name, '_page_', 2)::int as page_number,\n    '@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO' as stage_prefix,\n    to_file(file_url) as pdf_file,\n    parse_json(\n        to_varchar(\n            snowflake.cortex.parse_document(\n                '@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO',\n                file_name,\n                {'mode': 'LAYOUT'}\n            )\n        )\n    ):content as pdf_text\nfrom\n    directory(@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO)\nwhere\n    relative_path like '%paged_pdf/%'\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ee157c4-cc09-427e-b433-b74eb96de349",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "select * from pdf_pages;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0ae7050-8372-48d3-a4b8-a38b06ce306a",
   "metadata": {
    "name": "MD_metadata_chunking",
    "collapsed": false
   },
   "source": "## 📖 Enriching PDF Pages with Metadata and Text Chunks for Semantic Search\n\nThis SQL pipeline creates a comprehensive table (`utils.ai.pdf_images_joined`) that combines page-level text, image embeddings, structured metadata, and semantically formatted chunks optimized for multimodal document retrieval using Snowflake Cortex.\n\n### ✅ Steps:\n\n1. **🖇️ Join PDF Pages with Image Embeddings**\n\n   * Merges parsed PDF page data from `pdf_pages` with vector embeddings from `output_vector_table` via `paged_file_name`.\n---\n2. **📄 Select Representative Pages for Metadata**\n\n   * Uses `row_number()` to select:\n\n     * The **first 10 pages** (for coverage of typical document headers).\n     * The **last 2 pages** (often contain part indexes or summaries).\n---\n3. **🧠 Generate Document-Level Metadata**\n\n   * Concatenates the selected pages’ text and feeds it into `ai_complete()` (with `llama4-scout`) to extract:\n\n     * `manufacturer`\n     * `product_line`\n     * `document_type`\n     * `effective_date`, `copyright`\n     * `category`\n     * `concise_document_summary`\n---\n4. **📝 Generate Page-Level Metadata**\n\n   * Runs `ai_complete()` (with `llama4-scout`) on each page’s text to extract:\n\n     * `page_title`\n     * `concise_page_summary`\n---\n5. **🔗 Join Metadata with Full Page Content**\n\n   * Combines document-level and page-level metadata with:\n\n     * Raw page text\n     * Vector embeddings\n     * File references\n---\n6. **✂️ Split Pages into Chunks**\n\n   * Uses `cortex.split_text_recursive_character()` to break page text into \\~1800-character, markdown-safe blocks, ensuring semantic cohesion for chunk-level retrieval.\n---\n7. **🔍 Enrich Chunks with Visual Context**\n\n   * For each chunk:\n\n     * Runs `ai_complete()` (with `llama4-maverick`) using the **full page image** and **chunk text**.\n     * Extracts structured visual context such as:\n\n       * **Page region**\n       * **Table sections or headers**\n       * **Related elements not captured in the chunk**\n     * Encourages bullet-point or key-value output grounded in visual layout.\n---\n8. **🧱 Build Final Enriched Chunks**\n\n   * Combines:\n\n     * Source file\n     * Document and page metadata\n     * Chunk visual context\n     * Raw chunk text\n   * Stores final result in an `enriched_chunk` field optimized for LLM prompts and semantic indexing."
  },
  {
   "cell_type": "code",
   "id": "b905e893-f090-4b3e-aabb-6bbbc6c09b90",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": " select\n        pdfs.file_name as pdf_file_name,\n        images.file_name as image_file_name,\n        pdfs.original_file_name,\n        pdfs.page_number,\n        pdfs.pdf_file,\n        images.image_file,\n        images.image_vector,\n        pdfs.pdf_text,\n    from\n        pdf_pages as pdfs\n    join\n        output_vector_table as images\n        on\n        pdfs.paged_file_name = images.paged_file_name",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f4e2d39-332d-4abc-89f5-c132dc885868",
   "metadata": {
    "language": "sql",
    "name": "SQL_metadata_chunking"
   },
   "outputs": [],
   "source": "create or replace table pdf_images_joined as\nwith pdf_images_joined as (\n    select\n        pdfs.file_name as pdf_file_name,\n        images.file_name as image_file_name,\n        pdfs.original_file_name,\n        pdfs.page_number,\n        pdfs.pdf_file,\n        images.image_file,\n        images.image_vector,\n        pdfs.pdf_text,\n    from\n        pdf_pages as pdfs\n    join\n        output_vector_table as images\n        on\n        pdfs.paged_file_name = images.paged_file_name\n),\nfirst_10_pages as (\n  select\n    original_file_name,\n    page_number,\n    pdf_text,\n    row_number() over (partition by original_file_name order by page_number) as row_num_start,\n    row_number() over (partition by original_file_name order by page_number desc) as row_num_end\n  from\n    pdf_images_joined\n),\nlimited_pages as (\n  select\n    original_file_name,\n    page_number,\n    pdf_text\n  from \n    first_10_pages\n  where \n    row_num_start <= 10\n    or\n    row_num_end <= 2\n),\ndocument_text as (\n  select\n    original_file_name,\n    listagg(pdf_text, '\\n\\n') within group (order by page_number) as full_text\n  from \n    limited_pages\n  group by \n    original_file_name\n),\nget_document_summary as (\n    select\n      original_file_name,\n      full_text,\n      ai_complete(\n        model => 'llama4-scout',\nprompt => concat(\n  'You are analyzing a financial fact book containing statistical data, trends, and analytics about investment companies and funds. ',\n  'This document contains industry statistics, market data, performance metrics, and regulatory information that will be indexed for financial analysis and research queries.\\n\\n',\n  'Extract key identifying information that would help financial professionals, researchers, and analysts find this document when searching for investment industry data. ',\n  'Focus on:\\n',\n  '- What types of financial data and statistics are covered\\n',\n  '- Time periods and geographic scope of the data\\n',\n  '- What investment sectors, fund types, or market segments are analyzed\\n',\n  '- What regulatory or industry context is provided\\n\\n',\n  'Rules:\\n',\n  '1. Focus on financial data categories, market segments, and statistical scope\\n',\n  '2. Use simple key:value format, one per line\\n',\n  '3. Only include information you are confident about\\n',\n  '4. Return ONLY the key:value pairs\\n\\n',\n  'Extract these fields if present:\\n',\n  array_to_string(\n      array_construct(\n          'document_type',\n          'publication_year',\n          'data_time_period_covered',\n          'geographic_scope',\n          'fund_types_covered',\n          'statistical_focus_areas',\n          'regulatory_context',\n          'target_audience',\n          'primary_data_sources'\n      ),\n      '\\n\\t* '\n  ),\n  '\\n\\nDocument content:\\n', full_text, '\\n\\n'\n),\n        model_parameters => {\n          'temperature': 0.2\n        }\n      )::string as document_metadata\n    from \n        document_text\n),\ndescribe_pages as (\n    select\n        pdf_file_name,\n        image_file_name,\n        original_file_name,\n        page_number,\n        pdf_file,\n        image_file,\n        image_vector,\n        pdf_text,\n        ai_complete(\n          model => 'llama4-scout',\nprompt => concat(\n  'You are analyzing individual pages from a financial fact book containing investment industry statistics, market data, and performance analytics. ',\n  'Each page may contain statistical tables, trend graphs, market analysis, or regulatory information about investment funds and companies.\\n\\n',\n  'Extract page-level metadata that would help financial professionals find specific data points, trends, or market segments. ',\n  'Focus on what financial metrics, time periods, fund categories, or market data are covered on this specific page.\\n\\n',\n  'Rules:\\n',\n  '1. Identify the main financial data focus or market segment on this page\\n',\n  '2. Use simple key:value format, one per line\\n',\n  '3. Be specific about metrics, time periods, and fund types covered\\n',\n  '4. Return ONLY the key:value pairs\\n\\n',\n  'Extract these fields if present:\\n',\n  array_to_string(\n    array_construct(\n      'page_data_focus',\n      'financial_metrics_covered',\n      'time_period_analyzed',\n      'fund_categories_or_market_segments',\n      'key_statistical_highlights'\n    ),\n    '\\n\\t* '\n  ), '\\n\\n',\n  'Page content:\\n', pdf_text, '\\n\\n'\n),\n          model_parameters => {\n            'temperature': 0.1,\n            'max_tokens': 1024\n          }\n        )::string as page_metadata\n    from\n        pdf_images_joined\n),\npages_with_metadata as (\n  select\n    page.pdf_file_name,\n    page.image_file_name,\n    page.original_file_name,\n    page.page_number,\n    page.pdf_file,\n    page.image_file,\n    page.image_vector,\n    page.pdf_text,\n    page.page_metadata,\n    doc.document_metadata\n  from\n    describe_pages page\n  join\n    get_document_summary doc\n    on \n        page.original_file_name = doc.original_file_name\n),\nsplit_pages_into_chunks as (\n    select\n        pdf_file_name,\n        image_file_name,\n        original_file_name,\n        page_number,\n        image_vector,\n        pdf_text,\n        document_metadata,\n        page_metadata,\n        ai_complete(\n            model => 'llama4-maverick',\npredicate => concat(\n    'You are analyzing financial fact book images alongside extracted text chunks containing statistical data, graphs, and market analytics. ',\n    'Your goal is to describe how this text chunk relates to the visual elements (charts, tables, graphs) and overall financial data presentation shown in the image.\\n\\n',\n    \n    'You will see:\\n',\n    '- A page image containing financial charts, statistical tables, or data visualizations\\n',\n    '- A text chunk extracted from that page\\n\\n',\n    \n    'Describe the chunk''s role in the overall financial data presentation:\\n\\n',\n    \n    'For statistical tables and data, focus on:\\n',\n    '1. Data context: what financial metrics or fund categories are being measured\\n',\n    '2. Time periods: what years, quarters, or time ranges are covered\\n',\n    '3. Comparative elements: year-over-year changes, benchmarks, or trend indicators\\n',\n    '4. Geographic scope: domestic vs international, regional breakdowns\\n\\n',\n    \n    'For charts and graphs, focus on:\\n',\n    '1. Chart type: trend lines, bar charts, pie charts, performance comparisons\\n',\n    '2. Data relationships: correlations, growth patterns, market share distributions\\n',\n    '3. Key metrics: assets under management, returns, flows, expense ratios\\n',\n    '4. Visual indicators: legends, axes labels, data series\\n\\n',\n    \n    'For narrative sections, focus on:\\n',\n    '1. Market context: industry trends, regulatory changes, economic factors\\n',\n    '2. Data interpretation: explanations of statistical findings or trends\\n',\n    '3. Methodology: how data was collected, calculated, or analyzed\\n',\n    '4. Key insights: notable findings, industry implications, or future outlook\\n\\n',\n    \n    'Guidelines:\\n',\n    '* Be concise and use bullet points\\n',\n    '* Focus on financial data relationships and statistical context\\n',\n    '* Identify specific metrics, time periods, and fund categories\\n',\n    '* Reference visual elements that provide context to the data\\n\\n',\n    \n    'Text chunk:\\n', value::string, '\\n\\n'\n),\n            file => image_file,\n          model_parameters => {\n            'temperature': 0.1,\n            'max_tokens': 1024\n          }\n        )::string as chunk_context,\n        concat(\n            '**Source File:** ', original_file_name, '\\n',\n            '**Document Metadata:** ', coalesce(document_metadata, 'N/A'), '\\n\\n',\n            '----------------\\n\\n',\n            '**Page Metadata:** ', coalesce(page_metadata, 'N/A'), '\\n',\n            '**Page Number:** ', page_number::string, '\\n\\n',\n            '----------------\\n\\n',\n            '**Chunk Context:**', chunk_context,\n             '\\n\\n----------------\\n\\n',\n            '**Chunk:**\\n\\n', value::string\n        ) as enriched_chunk\n    from\n        pages_with_metadata,\n    lateral flatten(\n        input=>snowflake.cortex.split_text_recursive_character(\n            pdf_text,\n            'markdown',\n            1800,\n            200\n        )\n    )\n)\nselect\n    pdf_file_name,\n    image_file_name,\n    original_file_name,\n    page_number,\n    image_vector,\n    pdf_text,\n    enriched_chunk\nfrom\n    split_pages_into_chunks",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1122a28-4e18-4f21-a398-cc026236a8b5",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "select * from pdf_images_joined;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6bc27bae-b050-45c3-9b60-b6271bc4971a",
   "metadata": {
    "name": "MD_cortex_search",
    "collapsed": false
   },
   "source": "## 🕵️‍♀️ Build Cortex Search Service\n\nWe're using the [User-Provided Vector Embeddings in Cortex Search](https://docs.snowflake.com/LIMITEDACCESS/cortex-search/user-provided-vectors) private preview. This allows us to provide precomputed vector embeddings to index and query with Cortex Search, which allows us to use our image embeddings as part of the search service."
  },
  {
   "cell_type": "code",
   "id": "8f79e763-5106-42a8-b138-f092391a69de",
   "metadata": {
    "language": "sql",
    "name": "SQL_cortex_search"
   },
   "outputs": [],
   "source": "create or replace cortex search service docs_search_service\n    text indexes (enriched_chunk,pdf_text)\n    vector indexes (image_vector)\n    warehouse = CORTEX_SEARCH_TUTORIAL_WH\n    target_lag = '1 day'\n    as \n    select \n        pdf_file_name,\n        image_file_name,\n        original_file_name,\n        page_number,\n        image_vector,\n        pdf_text::varchar as pdf_text,\n        enriched_chunk\n    from \n       pdf_images_joined\n;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c1630e0-2ed4-493d-869a-0e719b7fa2a7",
   "metadata": {
    "name": "MD_perform_rag",
    "collapsed": false
   },
   "source": "## 🤖 Multimodal Document Question Answering with Image-Aided Semantic Search\n\nThis system answers precise technical questions about door hardware products using both **text** and **image-based** reasoning across PDFs. It completes a three-step multimodal process:\n\n1. **Text-Based Answering** using enriched document chunks.\n2. **Image-Based Answer Validation** via OCR page images.\n3. **Answer Synthesis** to revise and finalize the response.\n\nThis is done using the following steps:\n\n### 1. **Convert User Question into Image Embedding**\n\n* `get_text_embedding_via_image()`:\n\n  * Renders the user's question as a temporary PNG image.\n  * Uploads it to a Snowflake stage (`@utils.ai.stock_ikb_documents/queries/`).\n  * Uses `snowflake.cortex.embed_image_1024()` to generate a **multimodal embedding** via `voyage-multimodal-3`.\n\n---\n\n### 2. **Perform Semantic Search with Text + Embedding**\n\n* `query_search_service()`:\n\n  * Submits both the raw question and the image embedding to a **Cortex search service**.\n  * Returns the top 50 `ENRICHED_CHUNK`s (text + metadata + image references) relevant to the question.\n\n---\n\n### 3. **Rephrase Question for Search Compatibility**\n\n* `rephrase_for_search()`:\n\n  * Normalizes the user query by trimming and lowercasing it.\n  * Helps align question formatting with indexed content.\n\n---\n\n### 4. **Build Context and Generate Text-Based Answer**\n\n* `ai_complete_on_text()`:\n\n  * Constructs a markdown-rich prompt from the top retrieved chunks.\n  * Includes clickable links to the original PDF pages via `presigned_url`.\n  * Uses `claude-3-7-sonnet` to answer the question directly and include:\n\n    * A clear **direct answer**,\n    * A **confidence score** (0–1),\n    * A short **justification**, and\n    * Properly formatted **CITED SOURCES**:\n      [`Document Name - page X`](presigned_url)\n\n---\n\n### 5. **Deduplicate and Filter Pages for Image Critique**\n\n* Duplicates are removed based on `(PDF_FILE_NAME, IMAGE_FILE_NAME)` pairs.\n* `extract_cited_docs_and_pages()` ensures only **relevant document pages** are processed.\n* `extract_page_number()` helps match images to cited pages.\n\n---\n\n### 6. **Run Image-Based Validations (Async)**\n\n* `ai_complete_on_image_async()`:\n\n  * Submits each matched image to `claude-3-7-sonnet` via `ai_complete()` using:\n\n    * Document metadata\n    * Page number\n    * The original text answer for critique\n  * Prompt instructs the model to confirm or revise the answer, ensuring it's grounded in the visual page content.\n\n* `resolve_async_job()`:\n\n  * Polls the result and extracts fields like:\n\n    * `RESULT`, `PAGE_NUMBER`, `IMAGE_FILE_NAME`, `PRESIGNED_URL`.\n\n---\n\n### 7. **Filter Image-Based Answers by Confidence**\n\n* `filter_by_confidence()`:\n\n  * Retains only image completions with `CONFIDENCE >= 0.5`.\n  * Helps ensure only high-quality critiques contribute to the final answer.\n\n---\n\n### 8. **Synthesize the Final Answer for the User**\n\n* `synthesise_all_answers()`:\n\n  * Merges text and image critiques into a unified prompt.\n  * Final LLM prompt includes:\n\n    * Original text result\n    * All image critiques (linked to page images)\n  * The LLM is instructed to:\n\n    * Revise or reaffirm the answer\n    * Rephrase for user clarity (customer-facing tone)\n    * Exclude technical fields like `CONFIDENCE` or `JUSTIFICATION`\n    * Append a **\"Cited Sources\"** section with markdown hyperlinks\n\n---"
  },
  {
   "cell_type": "code",
   "id": "ac3a442c-1d03-4045-8314-570caf933df2",
   "metadata": {
    "language": "python",
    "name": "PY_perform_rag"
   },
   "outputs": [],
   "source": "# def query_search_service(session, my_service, query_text):\n#     query_embedding = get_text_embedding_via_image(session, query_text)\n#     resp = my_service.search(\n#         query = query_text,\n#         experimental = {\n#             \"QueryEmbedding\": query_embedding\n#         },\n#         columns=[\n#             \"ENRICHED_CHUNK\",\n#             \"PDF_FILE_NAME\",\n#             \"IMAGE_FILE_NAME\",\n#             \"ORIGINAL_FILE_NAME\",\n#             \"PAGE_NUMBER\"\n#         ],\n#         limit=5\n#     )\n#     return resp.to_json()\n \n\ndef query_multi_index_search_service(session, my_service, query_text):\n    query_embedding = get_text_embedding_via_image(session, query_text)\n    \n    resp = my_service.search(\n        # Use ONLY multi_index_query, not both query and multi_index_query\n        multi_index_query={\n            \"image_vector\": [\n                {\"vector\": query_embedding}\n            ],\n            # \"enriched_chunk\":[{\"text\":query_text}]\n            # ,\n            \"pdf_text\":[{\"text\":query_text}]},\n        \n        columns=[\n            \"ENRICHED_CHUNK\",\n            \"PDF_FILE_NAME\",\n            \"IMAGE_FILE_NAME\",\n            \"ORIGINAL_FILE_NAME\",\n            \"PAGE_NUMBER\"\n        ],\n        limit=20\n    )\n    \n    return resp.to_json()\n\n# def query_multi_index_search_service(session, my_service, query_text):\n#     query_embedding = get_text_embedding_via_image(session, query_text)\n    \n#     resp = my_service.search(\n#         # Use ONLY multi_index_query, not both query and multi_index_query\n#         multi_index_query={\n#             \"image_vector\": [\n#                 {\"vector\": query_embedding}\n#             ],\n#         },\n        \n#         columns=[\n#             \"ENRICHED_CHUNK\",\n#             \"PDF_FILE_NAME\",\n#             \"IMAGE_FILE_NAME\",\n#             \"ORIGINAL_FILE_NAME\",\n#             \"PAGE_NUMBER\"\n#         ],\n#         limit=20\n#     )\n    \n#     return resp.to_json()\n\ndef create_temp_image_from_text(text: str) -> tuple[str, str]:\n    query_hash = hashlib.md5(text.strip().lower().encode()).hexdigest()\n    image_filename = f\"{query_hash}.png\"\n\n    temp_file = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n    file_path = temp_file.name\n    temp_file.close()\n\n    image = Image.new(\"RGB\", (1000, 200), \"white\")\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.load_default()\n    draw.text((10, 10), text, fill=\"black\", font=font)\n    image.save(file_path)\n\n\n    return file_path, image_filename\n\n\ndef extract_cited_docs_and_pages(text_answer_str):\n    cited = {}\n    matches = re.findall(r\"CITED SOURCES:\\s*(.+)\", text_answer_str, re.IGNORECASE)\n    for match in matches:\n        doc_page_pairs = re.findall(r\"([a-zA-Z0-9._ -]+?)\\s*-\\s*page\\s*(\\d+)\", match)\n        for doc, page in doc_page_pairs:\n            doc = doc.strip().lower()\n            page = page.strip()\n            cited.setdefault(doc, set()).add(page)\n    return cited\n\ndef extract_page_number(image_file_name: str) -> str:\n    match = re.search(r'_page_(\\d+)\\.png$', image_file_name)\n    return match.group(1) if match else \"N/A\"\n\n\ndef file_exists_in_stage(session, stage_name: str, file_path: str) -> bool:\n    result = session.sql(f\"list @{stage_name}/{file_path}\").collect()\n    return bool(result)\n\n\ndef fuzzy_match(a, b, threshold=0.6):\n    return SequenceMatcher(None, a.lower(), b.lower()).ratio() >= threshold\n\n\ndef upload_file_to_stage(session, local_path: str, stage_name: str, dest_file_name: str):\n    temp_dir = tempfile.gettempdir()\n    temp_named_path = os.path.join(temp_dir, dest_file_name)\n\n    os.makedirs(os.path.dirname(temp_named_path), exist_ok=True)\n    shutil.copyfile(local_path, temp_named_path)\n\n    try:\n        result = session.file.put(\n            temp_named_path,\n            f\"@{stage_name}/queries\",\n            overwrite=True,\n            auto_compress=False\n        )\n        \n    finally:\n        os.remove(temp_named_path)\n\n\ndef get_text_embedding_via_image(\n    session, \n    text: str, \n    stage_name=\"@cortex_search_tutorial_db.public.doc_repo\"\n):\n    temp_path, image_filename = create_temp_image_from_text(text)\n    stage_subpath = f\"queries/{image_filename}\"\n\n    try:\n        if not file_exists_in_stage(session, stage_name.lstrip(\"@\"), stage_subpath):\n            upload_file_to_stage(session, temp_path, stage_name.lstrip(\"@\"), stage_subpath)\n            \n        query = f\"\"\"\n            select \n                AI_EMBED(\n                    'voyage-multimodal-3', \n                    '{stage_name}+{stage_subpath.lstrip('/')}'\n                )\n        \"\"\"\n        embedding = session.sql(query).collect()[0][0]\n    finally:\n        os.remove(temp_path)\n\n    return embedding\n\n\ndef resolve_async_job(job):\n    try:\n        row = job.result()[0].asDict()\n        return {\n            \"RESULT\": row[\"RESULT\"],\n            \"ORIGINAL_FILE_NAME\": row[\"ORIGINAL_FILE_NAME\"],\n            \"IMAGE_FILE_NAME\": row[\"IMAGE_FILE_NAME\"],\n            \"PRESIGNED_URL\": row.get(\"PRESIGNED_URL\", \"#\")\n        }\n    except Exception as e:\n        return {\n            \"RESULT\": f\"Error: {e}\",\n            \"ORIGINAL_FILE_NAME\": None,\n            \"IMAGE_FILE_NAME\": None,\n            \"PRESIGNED_URL\": \"#\"\n        }\n\n\ndef rephrase_for_search(question):\n    return question.strip().lower()\n\n\ndef filter_by_confidence(responses, threshold=0.5):\n    filtered = []\n    for item in responses:\n        match = re.search(r\"CONFIDENCE:\\s*([0-9.]+)\", item[\"RESULT\"], re.IGNORECASE)\n        score = float(match.group(1)) if match else 0.0\n        if score >= threshold:\n            filtered.append(item)\n    return filtered\n\n\ndef sql_escape(value):\n    return str(value).replace(\"'\", \"''\") if value is not None else \"\"\n\n\ndef run_model(model_name, llm_prompt, session, temperature, max_tokens, top_p, guardrails, stream):\n    return complete(\n        model=model_name,\n        prompt=[{\"role\": \"user\", \"content\": llm_prompt}],\n        session=session,\n        options=CompleteOptions(\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            guardrails=guardrails\n        ),\n        stream=stream\n    )\n\n\ndef ai_complete_on_text(session, question, retrieved_chunks):\n    seen = set()\n    enriched_context_blocks = []\n\n    for chunk in retrieved_chunks:\n        enriched_chunk = chunk[\"ENRICHED_CHUNK\"]\n        original_file = chunk.get(\"ORIGINAL_FILE_NAME\")\n        image_file = chunk.get(\"IMAGE_FILE_NAME\")\n\n        if not image_file or not original_file:\n            continue\n\n        key = (original_file, image_file, enriched_chunk)\n        if key in seen:\n            continue\n        seen.add(key)\n\n        # Generate presigned URL\n        presigned_url = session.sql(\n            f\"SELECT GET_PRESIGNED_URL(@cortex_search_tutorial_db.public.doc_repo, '{sql_escape(image_file)}')\"\n        ).collect()[0][0]\n\n        # Format for the model\n        block = dedent(f\"\"\"\n        ---\n        📄 **Source**: [{original_file}]({presigned_url})\n        📜 **Extracted Content**:\n        {enriched_chunk}\n        \"\"\").strip()\n\n        enriched_context_blocks.append(block)\n\n    if not enriched_context_blocks:\n        return {\"result\": \"No usable context.\", \"metadata\": {}}\n\n    full_context = \"\\n\\n\".join(enriched_context_blocks)\n\n    prompt = dedent(f\"\"\"\n    You are a financial data analysis assistant helping users answer precise questions \n    about investment company statistics, fund performance, and market trends based on \n    structured financial fact book data (e.g., statistical tables, performance charts, regulatory reports).\n    \n    ---\n    \n    ## Your Task:\n    Answer the user's question using only the provided context. \n    Each context block includes extracted data from one document page containing \n    financial statistics, market data, or regulatory information.\n    \n    ---\n    ## Precision Requirements:\n    - Only use data points that match **all explicit constraints** \n        in the question (e.g., fund type, time period, geographic scope, specific metrics, etc.).\n    - Do not include unrelated fund categories or time periods \n        (e.g., do not include \"international funds\" if the question is about \"domestic equity funds\").\n    - If data is ambiguous, incomplete, or methodology is unclear, say so rather than guessing.\n    - Avoid over-including statistics or metrics that don't clearly satisfy the question.\n    - When multiple data sources exist, prioritize the most recent or comprehensive dataset.\n    - Always specify time periods, fund categories, and geographic scope in your answer.\n    - Cited sources MUST include hyperlinks to the original document using the presigned_url\n    \n    ---\n    ## Response Format:\n    - Start with a **DIRECT ANSWER** — a clear response to the question (specific values, percentages, trends, YES/NO, etc.).\n    - Then include:\n      - `CONFIDENCE:` (a float from 0.0 to 1.0)\n      - `JUSTIFICATION:` why this answer is supported by the statistical data\n      - `CITED SOURCES:` in the form [`Document Name - page X`](presigned_url)\n    \n    ---\n    \n    ## Question:\n    {question.strip()}\n    \n    ## Context:\n    {full_context}\n    \n    ---\n    \n    ## Output Format:\n    DIRECT ANSWER:  \n    CONFIDENCE:  \n    JUSTIFICATION:  \n    CITED SOURCES:\n    \"\"\")\n\n\n    result = complete(\n        model=\"mistral-large2\",\n        prompt=[{\"role\": \"user\", \"content\": prompt}],\n        session=session,\n        options=CompleteOptions(\n            temperature=0,\n            max_tokens=2048,\n            top_p=1,\n            guardrails=False\n        ),\n        stream=False\n    )\n\n    return {\n        \"result\": \"\".join(result),\n        \"metadata\": {\n            \"source\": \"TEXT\",\n            \"num_chunks\": len(retrieved_chunks)\n        },\n        \"prompt\": prompt\n    }\n\n\ndef ai_complete_on_image_async(session, question, item, text_answer):\n    image_file_name = item[\"IMAGE_FILE_NAME\"]\n    original_file_name = item.get(\"ORIGINAL_FILE_NAME\", \"\")\n    page_number = item.get(\"PAGE_NUMBER\", \"\")\n\n    # Escape for SQL\n    image_file_escaped = sql_escape(image_file_name)\n    original_file_escaped = sql_escape(original_file_name)\n    document_metadata_escaped = sql_escape(original_file_name)\n    page_metadata_escaped = sql_escape(str(page_number))\n    answer_snippet_escaped = sql_escape(text_answer[\"result\"][:2000])  # trim long strings for prompt\n\n    prompt = dedent(f\"\"\"\n    You are validating a textual answer using the actual document page image.\n\n    ---\n    **User Question**: {question.strip()}\n\n    ---\n    **Answer to Critique**:\n    {text_answer[\"result\"]}\n\n    ---\n    **Image Source**: Document: `{original_file_name}`, Page: {page_number}\n\n    Your job is to check if the provided answer is correct and properly grounded in the image.\n    - Identify any factual errors, missing context, or overclaims.\n    - Be strict: only endorse what is clearly present in the image.\n    - If the image is not useful for critiquing the answer then say so.\n    - If the response is incorrect, provide the correct answer.\n    \n    Output format:\n    CRITIQUE_RESULT:  \n    JUSTIFICATION:  \n    CITED SOURCES:\n    \"\"\")\n\n    prompt_escaped = prompt.replace(\"'\", \"\\\\'\")\n\n    df = session.sql(f\"\"\"\n        select \n            '{original_file_escaped}' as original_file_name,\n            '{image_file_escaped}' as image_file_name,\n            '{document_metadata_escaped}' as document_metadata,\n            '{page_metadata_escaped}' as page_metadata,\n            get_presigned_url('@utils.ai.stock_ikb_documents', '{image_file_escaped}') as presigned_url,\n            ai_complete(\n                'pixtral-large',\n                '{prompt_escaped}',\n                to_file('@utils.ai.stock_ikb_documents', '{image_file_escaped}'),\n                object_construct('temperature', 0.2, 'top_p', 1.0, 'max_tokens', 2048, 'guardrails', FALSE)\n            ) as result\n    \"\"\")\n    return df.collect_nowait()\n\n\ndef synthesise_all_answers(session, question, text_answer_dict, image_answer_dicts):\n    text_result = text_answer_dict[\"result\"]\n    text_meta = text_answer_dict.get(\"metadata\", {})\n\n    image_sections = []\n    for img in image_answer_dicts:\n        presigned_link = img.get(\"PRESIGNED_URL\", \"#\")\n        section = dedent(f\"\"\"\n        --- \n        📄 **Source**: [{img[\"ORIGINAL_FILE_NAME\"]}]({presigned_link})\n        🖼️ Image File: `{img[\"IMAGE_FILE_NAME\"]}`\n\n        📘 Page Metadata:\n        {img.get(\"PAGE_METADATA\", \"N/A\")}\n\n        📚 Document Metadata:\n        {img.get(\"DOCUMENT_METADATA\", \"N/A\")}\n\n        📌 Critique of Text Answer:\n        {img[\"RESULT\"]}\n        \"\"\")\n        image_sections.append(section.strip())\n\n    image_critique_block = \"\\n\\n\".join(image_sections)\n\n    prompt = dedent(f\"\"\"\\\n    You are refining a text-based answer using critiques from document image analysis.\n\n    ## User Question:\n    {question}\n\n    ## Original Text Answer:\n    {text_result}\n\n    ## Image-Based Critiques:\n    {image_critique_block}\n\n    ---\n    ## Final Synthesized Answer:\n    - Revise the original text answer if needed.\n    - Improve factual accuracy using the critiques.\n    - Remove anything unsupported.\n    - Clearly cite updated sources.\n    - Final answer should be **concise, accurate, and directly responsive** to the user's question.\n    - Rephrase the technical answer so it's clear and understandable to the user.\n    - Do **not** include system fields like CONFIDENCE or JUSTIFICATION.\n    - Instead, write the answer as if you were explaining it to a knowledgeable customer.\n    - At the end, include a **Cited Sources** section with **Markdown hyperlinks** using the provided list.\n\n    ## Final Output:\n    - ANSWER:\n    - CITED SOURCES: \n        - [<file_name> - page <number>](url)\n    \"\"\")\n\n    result = complete(\n        model=\"mistral-large2\",\n        prompt=prompt,\n        session=session,\n        options=CompleteOptions(\n            temperature=0.2, max_tokens=2048, top_p=1.0, guardrails=False\n        ),\n        stream=False\n    )\n\n    return \"\".join(result)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a3359b5-43db-4b12-994a-c538445dfcdf",
   "metadata": {
    "name": "MD_streamlit_interface",
    "collapsed": false
   },
   "source": "## 📘 Streamlit App: Seclock Multimodal Q\\&A Workflow\n\nThis app provides an interactive interface for querying technical door hardware documents using a **multimodal pipeline**—combining semantic search, text comprehension, image-based validation, and answer synthesis.\n\n### 🔢 **Workflow Overview**\n\nThe pipeline consists of **7 sequential steps**, shown with real-time status updates in the UI:\n\n---\n\n### 1. **Document Search (Cortex Semantic Search)**\n\n* The user enters a question via `st.chat_input()`.\n* The question is normalized using `rephrase_for_search()`.\n* `query_search_service()` performs an **embedding + keyword** search using the Cortex Search Service.\n* Returns up to 50 enriched document chunks with metadata, image filenames, and associated PDF references.\n* If no chunks are found, the assistant replies with a fallback message.\n\n---\n\n### 2. **Answer Generation from Text Chunks**\n\n* `ai_complete_on_text()` receives the retrieved chunks and builds a prompt with:\n\n  * **Clickable presigned URLs**\n  * Clean markdown formatting\n  * Strong precision constraints to avoid hallucinations\n* The model (`claude-3-7-sonnet`) returns:\n\n  * A **direct answer**\n  * A **confidence score**\n  * A **justification**\n  * **CITED SOURCES** with Markdown links\n\n---\n\n### 3. **Image Deduplication**\n\n* To reduce cost and noise, `(PDF_FILE_NAME, IMAGE_FILE_NAME)` pairs are deduplicated.\n* This ensures each page is only submitted once for image-based validation.\n\n---\n\n### 4. **Extract Cited Sources & Pages**\n\n* The model's output from Step 2 is parsed using `extract_cited_docs_and_pages()` to identify only the **relevant documents and pages** to check against image data.\n* Uses regex to extract all `Document - page X` citations for focused validation.\n\n---\n\n### 5. **Filter Images to Cited Pages**\n\n* The deduplicated images are **filtered** to those matching the cited documents and pages.\n* Matching is done via fuzzy string matching and page number extraction.\n* Each matched page is then submitted to Cortex with `ai_complete_on_image_async()` for image critique.\n\n---\n\n### 6. **Run Image-Based Validation (Async)**\n\n* Cortex jobs are resolved using `resolve_async_job()` and visual progress is displayed.\n* Errors are handled gracefully, and failed results are tagged with placeholders.\n* The results include `RESULT`, `PRESIGNED_URL`, and other metadata fields.\n\n---\n\n### 7. **Final Answer Synthesis**\n\n* All validated image responses are passed to `synthesise_all_answers()` along with the original text answer.\n* The synthesis prompt:\n\n  * Merges, reconciles, and improves factual accuracy.\n  * Rephrases the result clearly for human readers.\n  * **Cites each document+page with a working hyperlink**.\n* The result is then displayed to the user in the chat.\n\n---"
  },
  {
   "cell_type": "code",
   "id": "eea3ad13-7e91-407b-9d5f-c2eb38822aa4",
   "metadata": {
    "language": "python",
    "name": "PY_streamlit_interface"
   },
   "outputs": [],
   "source": "user_question = st.chat_input(\"Ask a question about financial docs...\")\n\nroot = Root(sp_session)\nsearch_service = (root\n  .databases[\"CORTEX_SEARCH_TUTORIAL_DB\"]\n  .schemas[\"PUBLIC\"]\n  .cortex_search_services[\"DOCS_SEARCH_SERVICE\"]\n)\n\nif user_question:\n    overall_start = time.time()\n    st.chat_message(\"user\").write(user_question)\n\n    steps = [\n        \"Search documents\",\n        \"Generate text answer\",\n        \"Deduplicate image chunks\",\n        \"Extract cited pages\",\n        \"Filter images to cited pages\",\n        \"Run image-based critique\",\n        \"Synthesize final answer\"\n    ]\n\n    with st.spinner(\"Thinking...\"):\n        # Init UI placeholders\n        status_step = st.empty()\n        status_stage = st.empty()\n        status_detail = st.empty()\n        status_progress = st.empty()\n        status_total_time = st.empty()\n\n        # --- Step 1: Search documents ---\n        step = 0\n        start = time.time()\n        status_step.markdown(f\"### 🔍 Step {step + 1} of {len(steps)}: {steps[step]}\")\n        status_stage.markdown(\"Searching document chunks using Cortex search...\")\n        search_query = rephrase_for_search(user_question)\n        ##raw_results = query_search_service(sp_session, search_service, search_query)\n        raw_results = query_multi_index_search_service(sp_session, search_service, search_query)\n        results = json.loads(raw_results)[\"results\"]\n        retrieved_chunks = results\n        status_detail.markdown(f\"Found `{len(retrieved_chunks)}` document chunks in `{time.time() - start:.2f}` seconds.\")\n\n        if not retrieved_chunks:\n            st.chat_message(\"assistant\").write(\"I couldn't find anything relevant in the documents.\")\n        else:\n            # --- Step 2: Text generation ---\n            step += 1\n            start = time.time()\n            status_step.markdown(f\"### 📝 Step {step + 1} of {len(steps)}: {steps[step]}\")\n            status_stage.markdown(\"Generating structured answer from retrieved text...\")\n            answer_text = ai_complete_on_text(sp_session, user_question, retrieved_chunks)\n            status_detail.markdown(f\"Answer generated in `{time.time() - start:.2f}` seconds.\")\n\n            # --- Step 3: Deduplicate images ---\n            step += 1\n            start = time.time()\n            status_step.markdown(f\"### 🧼 Step {step + 1} of {len(steps)}: {steps[step]}\")\n            status_stage.markdown(\"Removing duplicate images by document + page...\")\n            seen = set()\n            deduped_results = []\n            for item in results:\n                key = (item[\"PDF_FILE_NAME\"], item[\"IMAGE_FILE_NAME\"])\n                if key not in seen:\n                    seen.add(key)\n                    deduped_results.append(item)\n            status_detail.markdown(f\"Deduplicated to `{len(deduped_results)}` unique images in `{time.time() - start:.2f}` seconds.\")\n\n            # --- Step 4: Extract cited pages ---\n            step += 1\n            status_step.markdown(f\"### 📄 Step {step + 1} of {len(steps)}: {steps[step]}\")\n            status_stage.markdown(\"Parsing cited documents and pages from answer...\")\n            cited_docs_pages = extract_cited_docs_and_pages(answer_text[\"result\"])\n            status_detail.markdown(f\"Found `{sum(len(pgs) for pgs in cited_docs_pages.values())}` total cited pages.\")\n\n            # --- Step 5: Filter to cited pages ---\n            step += 1\n            start = time.time()\n            status_step.markdown(f\"### 🔎 Step {step + 1} of {len(steps)}: {steps[step]}\")\n            status_stage.markdown(\"Matching deduplicated images to cited documents and pages...\")\n\n            jobs = []\n            matched_files = []\n            skipped_files = []\n\n            for item in deduped_results:\n                original_file = item.get(\"ORIGINAL_FILE_NAME\", \"\")\n                base_name = os.path.splitext(original_file)[0].lower()\n                image_file = item.get(\"IMAGE_FILE_NAME\", \"\")\n                page_number = extract_page_number(image_file)\n\n                matched = any(\n                    cited_doc in base_name and page_number in cited_docs_pages[cited_doc]\n                    for cited_doc in cited_docs_pages\n                )\n\n                if matched:\n                    matched_files.append(f\"{base_name} - page {page_number}\")\n                    job = ai_complete_on_image_async(sp_session, user_question, item, answer_text)\n                    if job:\n                        jobs.append((item, job))\n                else:\n                    skipped_files.append(f\"{base_name} - page {page_number}\")\n\n            status_detail.markdown(f\"Matched `{len(matched_files)}` / `{len(deduped_results)}` images in `{time.time() - start:.2f}` seconds.\")\n\n            # --- Step 6: Run image-based critique ---\n            step += 1\n            start = time.time()\n            status_step.markdown(f\"### 🧠 Step {step + 1} of {len(steps)}: {steps[step]}\")\n            status_stage.markdown(\"Submitting matched images to Cortex for validation...\")\n\n            image_answers = []\n            total_jobs = len(jobs)\n            progress_bar = status_progress.progress(0.0, text=\"Processing image critiques...\")\n\n            for i, (item, job) in enumerate(jobs, start=1):\n                try:\n                    result = resolve_async_job(job)\n                    result[\"PAGE_NUMBER\"] = extract_page_number(item[\"IMAGE_FILE_NAME\"])\n                    image_answers.append(result)\n                except Exception as e:\n                    image_answers.append({\n                        \"RESULT\": f\"Error: {e}\",\n                        \"ORIGINAL_FILE_NAME\": item.get(\"ORIGINAL_FILE_NAME\"),\n                        \"IMAGE_FILE_NAME\": item.get(\"IMAGE_FILE_NAME\"),\n                        \"PAGE_NUMBER\": extract_page_number(item.get(\"IMAGE_FILE_NAME\", \"\")),\n                        \"PRESIGNED_URL\": \"#\"\n                    })\n                progress_bar.progress(i / total_jobs, text=f\"Processing image critiques... ({i}/{total_jobs})\")\n\n            status_detail.markdown(f\"Image critique completed in `{time.time() - start:.2f}` seconds.\")\n\n            # --- Step 7: Synthesize answer ---\n            step += 1\n            start = time.time()\n            status_step.markdown(f\"### 🧪 Step {step + 1} of {len(steps)}: {steps[step]}\")\n            status_stage.markdown(\"Synthesizing text + image answers into a final response...\")\n\n            image_answers_filtered = filter_by_confidence(image_answers)\n            final_answer = synthesise_all_answers(\n                sp_session,\n                user_question,\n                answer_text,\n                image_answers_filtered\n            )\n            status_detail.markdown(f\"Final synthesis completed in `{time.time() - start:.2f}` seconds.\")\n\n            # --- Display ---\n            total_time = time.time() - overall_start\n            status_total_time.markdown(f\"⏱️ **Total time taken**: `{total_time:.2f}` seconds\")\n            st.chat_message(\"assistant\").markdown(f\"**Final Answer:**\\n{final_answer}\")\n\n            with st.expander(\"🔍 Image Viewer\"):\n                ##image_path = f\"@CORTEX_SEARCH_TUTORIAL_DB.PUBLIC.DOC_REPO/PARSED/paged_image/2023-factbook_page_27.png\"\n                if results:    \n                    image_path = '@cortex_search_tutorial_db.public.doc_repo/PARSED/'+item.get(\"IMAGE_FILE_NAME\")\n                    ##print(image_path)\n                    try:\n                        image_st = sp_session.file.get_stream( image_path,decompress=False)\n                    except Exception as e:\n                        print(\"failed to initialize file stream:\", e)\n                        \n                    image = image_st.read()\n                    st.image(image)\n            \n            with st.expander(\"🔍 Debug - Raw Text Answer\"):\n                with st.chat_message(\"assistant\"):\n                    if results:\n                        st.write(\"📚 **Documents included:**\")\n                        for doc in sorted(set(item[\"ORIGINAL_FILE_NAME\"] for item in results)):\n                            st.write(f\"• `{doc}`\")\n                    st.markdown(\"---\")\n                    st.markdown(\"📝 **Text-Based Answer:**\")\n                    st.markdown(answer_text[\"result\"])\n\n            with st.expander(\"🔍 Debug - Raw Image Answers\"):\n                for i, ans in enumerate(image_answers, start=1):\n                    with st.chat_message(\"assistant\"):\n                        st.markdown(f\"**Image Answer {i}:**\")\n                        st.write(f\"📄 **Document**: `{ans['ORIGINAL_FILE_NAME']}`\")\n                        st.write(f\"🖼️ **Image File**: `{ans['IMAGE_FILE_NAME']}`\")\n                        st.markdown(ans[\"RESULT\"])\n\n\n            with st.expander(\"Prompt\"):\n                st.write(answer_text[\"prompt\"])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "06f582e5-bfba-4b2c-9c5d-bace5dcd897d",
   "metadata": {
    "name": "MD_groundtruth_results",
    "collapsed": false
   },
   "source": "## Groundtruth Questions\n\n1. What is the part number for a 10 line Y design cylinder?\n\n    **🔴 Seclock Answer**: 10-0613 left, 10-0614 right\n\n    **🤖 Solution Answer**:\n\n    > The part number for a 10 line Y design cylinder is 10-0613 (left-handed) or 10-0614 (right-handed). These are part of the Coastal Series and include both the cylinder and emergency release components.\n\n---\n2. Is chexit available on a 9947-F?\n\n    **🔴 Seclock Answer**: No, dogging is not an option on fire rated devices\n\n    **🤖 Solution Answer**:\n\n    > Yes, Chexit is available on a 9947-F device.\n    > \n    > The documentation confirms that Chexit (CX) is compatible with the 9947/9947-F model. This is shown in the minimum door opening sizes table for CX devices, which specifically lists \"CX9847/9847-F/9947/9947-F\" as compatible devices. Additionally, the Von Duprin price book states that \"CHEXIT - Available on all 98/99 Series devices,\" which includes the 9947-F model.\n    > \n    > When ordering, you would use the prefix \"CXA\" before the device model number to specify the Chexit option.\n\n---\n3. What is the case quantity and weight of a 281 Series O function?\n\n    **🔴 Seclock Answer**: Average weight is 24lbs, case quantity is 2\n\n    **🤖 Solution Answer**:\n    > The 281 Series O function has a case quantity of 2 and weighs 24 pounds per case.\n    > \n    > This information comes from the Sargent 2025 Price Book's Freight Weight Chart section, which specifically lists these specifications for the 281 Series.\n\n---\n4. On a Schlage ND, is vandlgard available in double cylinder functions?\n\n    **🔴 Seclock Answer**: Yes, functions 72 with XN12-003, 93 and 98\n\n    **🤖 Solution Answer**:\n    > Yes, Vandlgard is available in double cylinder functions on the Schlage ND Series. Specifically, it's available in these models:\n    > \n    > ND72 with XN12-003 (F80): Vandlgard communicating lock\n    > ND93 (F88): Vandlgard vestibule lock\n    > ND98 (F110): Vandlgard classroom security lock with 180-degree lockdown\n    > The Vandlgard feature allows the outside lever to rotate freely when locked, which helps prevent damage from vandalism by limiting the force that can be applied to the internal lock components.\n\n---\n5. What’s the part number for an L9460 armor front?\n\n    **🔴 Seclock Answer**: 09-665, you must also specify the finish\n\n    **🤖 Solution Answer**:\n\n    > The part number for an L9460 armor front is 09-665. This part is specifically designed for deadbolt configurations including the L9460 model.\n\n---\n6. What is the prefix for rail monitoring on a Sargent 80 device?\n\n    **🔴 Seclock Answer**: 55\n\n    **🤖 Solution Answer**:\n    > The prefix for rail monitoring on a Sargent 80 device is \"55-\". This prefix designates the Request-to-Exit option, which provides push rail monitoring functionality for various applications on Sargent's 80 Series exit devices. When ordering, you would include this prefix before the model number (for example: 55-8913F x ETMA x 32D x 36\").\n\n---\n7. What’s the list price for a Sargent 13-0404 cylinder plug size 44?\n\n    **🔴 Seclock Answer**: $44.44\n\n    **🤖 Solution Answer**: \n    \n    > The list price for a Sargent 13-0404 cylinder plug size 44 is $44.44.\n    \n---\n8. What finishes do Sargent mortise cylinder barrels coming in?\n    \n    **🔴 Seclock Answer**: US4, US15, and BSP\n\n    **🤖 Solution Answer**:\n\n    > Sargent mortise cylinder barrels come in three finishes: US4 (satin brass), US15 (satin nickel), and BSP (black suede powder coat).\n    > \n    > It's important to note that while the overall mortise cylinders are available in many different finishes (including US3, US9, US10, US10B, US10BE, US10BL, US20D, US32, US32D, BSP, WSP), the cylinder plugs/barrels specifically are only offered in these three finish options.\n\n---\n9. In ALX, what are the Less FSIC cylinder options?\n    \n    **🔴 Seclock Answer**: J-CO6 Less FSIC, made to fit Corbin 8000, J-CO7 Less FSIC, made to fit Corbin 8000-7, J-SAR Less FSIC, made to fit Sargent 6300\n\n    **🤖 Solution Answer**:\n    > In the ALX Series, the Less FSIC (Full Size Interchangeable Core) cylinder options are:\n    > \n    > * J-SAR: Less cylinder, made to fit Sargent 6300 6-pin FSIC\n    > * J-CO6: Less cylinder, made to fit Corbin 8000 6-pin FSIC\n    > * J-CO7: Less cylinder, made to fit Corbin 8000-7 7-pin FSIC\n    > \n    > These options allow you to use your existing FSIC cylinders with ALX Series locks, providing flexibility when integrating with existing key systems.\n\n---\n10. Is Schlage 7800 KE3/KE4 available with double cylinders?\n    \n    **🔴 Seclock Answer**: No, designed for exposed barrels only\n\n    **🤖 Solution Answer**:\n    \n    > No, the Schlage 7800 KE3/KE4 is not available with double cylinders. The product catalog specifically states that the KE3/KE4 escutcheon design cannot be used with double cylinder functions, which is an important limitation to note when considering this particular lock option."
  }
 ]
}